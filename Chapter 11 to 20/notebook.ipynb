{"cells":[{"cell_type":"markdown","id":"67879b97-e014-447d-943e-4d5ccd522197","metadata":{},"source":["# Intermediate Data Visualization with Seaborn"]},{"cell_type":"markdown","id":"40f5bdac-61ec-4c3c-b266-962fd8a69a78","metadata":{},"source":["## Seaborn histplot\n","- The **histplot** is similar to the histogram.\n","- By default, generates a histogram but can also generate other complex plots.\n","## Seaborn displot\n","- The **displot** levarages the **histplot** and other functions for distribution plots.\n","- By default, it generates a histogram but can also generate other plot types.\n","## Creating a histogram\n","- The **displot** function has multiple optional arguments.\n","- You can overlay a KDE plot on the histogram and specify the number of bins to use.\n","## Alternative data distributions \n","- A rug plot is an alternative way to view the distribution of data including small tickmarks along the x-axis.\n","- A kde curve and rug plot can be combined.\n","## Further plot types\n","- The displot function uses several functions including **kdeplot**, **rugplot** and **ecdfplot**.\n","- The **ecdfplot** shows the cumulative distribution of the data.\n","## Introducition to regplot\n","- The **regplot** function generates a scatter plot with a regression line.\n","- Usage is similar to the **displot**.\n","- The **data** and **x** and **y** variables must be defined.\n","## lmplot faceting\n","- Plotting multiple graphs while changing a single variable\n"]},{"cell_type":"code","execution_count":null,"id":"1a0bb2ab-339c-4e60-a8eb-1e56f9814495","metadata":{},"outputs":[],"source":["# Creating a histogram\n","\n","import seaborn as sns\n","sns.displot(df['alcohol'], kde=True, bins=10)\n","\n","# Alternative data distributions\n","\n","sns.displot(df['alcohol'], kind='kde', rug=True, fill=True)\n","\n","# Further plot types\n","\n","sns.displot(df['alcohol'], kind='ecdf')\n","\n","# Introduction to regplot\n","\n","sns.regplot(data=df, x=\"alcohol\", y=\"pH\")\n","\n","# lmplot() builds on top of the base regplot()\n","'''\n","- regplot (low level)\n","sns.regplot(data=df, \n","            x=\"alcohol\", \n","            y=\"quality\")\n","- lmplot (high level)\n","sns.lmplot(data=df, \n","           x=\"alcohol\", \n","           y=\"quality\")\n","They look similar but the second one is more powerful, it's more flexible.\n","'''\n","# lmplot faceting\n","'''\n","- Organize data by colors (hue)\n","sns.lmplot(data=df, \n","           x=\"quality\", \n","           y=\"alcohol\", \n","           hue=\"type\")\n","- Organize data by columns (col)\n","sns.lmplot(data=df, \n","           x=\"quality\", \n","           y=\"alcohol\", \n","           col=\"type\")\n","'''\n"]},{"cell_type":"markdown","id":"871d2778-9ec5-4593-a92f-88c409468c3c","metadata":{},"source":["## Setting Styles\n","- Seaborn has default configurations that can be applied with **sns.set()**.\n","- These styles can override matplotlib and pandas plots as well.\n","## Removing axes with despine()\n","- Sometimes plots are improved by removing elements.\n","- Seaborn contains a shortcut for removing the spines of a plot.\n","## Colors in Seaborn\n","- Seaborn supports assigning colors to plots using **matplotlib** color codes.\n","- Seaborn uses the **set_palette()** function to define a palette.\n","## Displaying Palettes\n","- **sns.palplot()** function displays a palette\n","- sns.color_palette\n","## Defining Custom Palettes\n","Circular colors = when the data is not ordered\n","- **sns.palplot(sns.color_palette(\"Paired\", 12))**\n","Sequential colors = when the data has a consistent range from high to low\n","- **sns.palplot(sns.color_palette(\"Blues\", 12))**\n","Diverging colors = when both the low and high values are interesting\n","- **sns.palplot(sns.color_palette(\"BrBG\", 12))**\n","## Customizing with matplotlib\n","- Most customization available through **matplotlib Axes** objects.\n","- **Axes** can be passed to seaborn functions.\n","- It is possible to combine and configure multiple plots."]},{"cell_type":"code","execution_count":1,"id":"9ce5ec0d-106d-47d1-803a-a5f79d36b824","metadata":{"executionTime":32,"lastSuccessfullyExecutedCode":""},"outputs":[],"source":["# Using seaborn styles\n","\n","sns.set()\n","df['Tuition'].plot.hist()\n","\n","for style in ['white','dark','whitegrid','darkgrid','ticks']:\n","    sns.set_style(style)\n","    sns.displot(df['Tuition'])\n","    plt.show()\n","\n","# Removing axes with despine()\n","\n","sns.set_style('white')\n","sns.displot(df['Tuition'])\n","sns.despine(left=True)\n","# The default is to remove the top and right lines\n","\n","# Colors in Seaborn\n","\n","sns.set(color_codes=True)\n","sns.displot(df['Tuition'], color='g')\n","\n","palettes = ['deep', 'muted', 'pastel', 'bright', 'dark','colorblind']\n","for p in palettes:\n","    sns.set_palette(p) \n","    sns.displot(df['Tuition'])\n","\n","# Displaying Palettes\n","\n","palettes = ['deep', 'muted', 'pastel', 'bright','dark','colorblind']\n","for p in palettes: \n","    sns.set_palette(p) \n","    sns.palplot(sns.color_palette()) \n","    plt.show()\n","\n","# Combining Plots\n","\n","fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, \n","                               sharey=True, figsize=(7,4))\n","\n","sns.histplot(df['Tuition'], stat='density', ax=ax0)\n","sns.histplot(df.query('State == \"MN\"')['Tuition'], stat='density', ax=ax1)\n","\n","ax1.set(xlabel='Tuition (MN)', xlim=(0, 70000))\n","ax1.axvline(x=20000, label='My Budget', linestyle='--')\n","ax1.legend()\n"]},{"cell_type":"markdown","id":"314d0477-fa5a-4667-91b7-cfa77f9caa88","metadata":{},"source":["## Categorical Data\n","Data which takes on a limited and fixed number of values, normally combined with numeric data.\n","Examples include:\n","- Geography (country, state, region)\n","- Gender\n","- Ethnicity\n","- Blood type\n","- Eye color\n","## Plot types\n","- **Show each observation**: the 1° category includes **stripplot()** and **swarmplot()**, showing all of the individual observations on the plot.\n","- **Abstract representations**: the 2° category contains **boxplot()**, **violinplot()** and **boxenplot()**, showing an abstract representation of the categorical data.\n","- **Statistical estimates**: the 3° category has **barplot()**, **pointplot()** and **countplot()**, showing statistical estimates of the categorical variables."]},{"cell_type":"code","execution_count":null,"id":"452c9dde-d0f9-4539-a3cb-95b2049d0140","metadata":{},"outputs":[],"source":["# Plots of each observation\n","\n","# Stripplot\n","\n","sns.stripplot(data=df, y=\"DRG Definition\", \n","              x=\"Average Covered Charges\", \n","              jitter=True)\n","# Swarmplot\n","\n","sns.swarmplot(data=df, \n","             y=\"DRG Definition\", \n","             x=\"Average Covered Charges\")\n","# Boxplot\n","\n","sns.boxplot(data=df, \n","            y=\"DRG Definition\", \n","            x=\"Average Covered Charges\")\n","# Violinplot\n","\n","sns.violinplot(data=df, \n","               y=\"DRG Definition\", \n","               x=\"Average Covered Charges\")\n","# Boxenplot\n","\n","sns.boxenplot(data=df, \n","              y=\"DRG Definition\", \n","              x=\"Average Covered Charges\")\n","# Barplot\n","\n","sns.barplot(data=df, \n","            y=\"DRG Definition\", \n","            x=\"Average Covered Charges\", \n","            hue=\"Region\")\n","# Pointplot\n","\n","sns.pointplot(data=df, \n","              y=\"DRG Definition\", \n","              x=\"Average Covered Charges\", \n","              hue=\"Region\")\n","# Countplot\n","\n","sns.countplot(data=df, \n","              y=\"DRG_Code\", \n","              hue=\"Region\")\n"]},{"cell_type":"markdown","id":"1b2d3136-fee4-478f-9986-95b98909615c","metadata":{},"source":["## Evaluating regression with residplot()\n","- A residual plot is useful for evaluating the fit of a model\n","- Ideally, the residual values in the plot should be plotted randomly across the horizontal line.\n","## Polynomial regression\n","- Seaborn supports polynimial regression using the **order** parameter.\n","## Estimators\n","- In some cases, an **x_estimator** can be useful for highlighting trends\n","## Getting data in the right format\n","- Seaborn's **heatmap()** function requires data to be in a grid format\n","- Pandas crosstab() is frequently used to manipulatethe data\n","## Pairwise relationships\n","- **PairGrid** shows pairwise relationships between two data elements\n","- The PairGrid supports defining the type of plots that can be displayed on the diagonals.\n"]},{"cell_type":"code","execution_count":null,"id":"149f9b64-3977-4d08-a495-e749aacdfa3b","metadata":{"executionCancelledAt":1679671157318,"executionTime":0,"lastSuccessfullyExecutedCode":"# valuating regression with residplot()\n\nsns.residplot(data=df, x='temp', y='total_rentals')\n\n# Polynomial regression\n\nsns.regplot(data=df, x='temp', y='total_rentals', order=2)\n\n# residplot with polynomial regression"},"outputs":[],"source":["# valuating regression with residplot()\n","\n","sns.residplot(data=df, x='temp', y='total_rentals')\n","\n","# Polynomial regression\n","\n","sns.regplot(data=df, x='temp', y='total_rentals',\n","            order=2)\n","\n","# residplot with polynomial regression\n","\n","sns.residplot(data=df, x='temp', y='total_rentals', \n","              order=2)\n","\n","# Categorical values\n","\n","sns.regplot(data=df, x='mnth', y='total_rentals', \n","            x_jitter=.1, \n","            order=2)\n","\n","# Estimators\n","\n","sns.regplot(data=df, x='mnth', y='total_rentals', \n","            x_estimator=np.mean, \n","            order=2)\n","\n","# Getting data in the right format\n","\n","pd.crosstab(df[\"mnth\"], df[\"weekday\"],\n","            values=df[\"total_rentals\"],aggfunc=\"mean\").round(0)\n","\n","# Build a heatmap\n","\n","sns.heatmap(pd.crosstab(df[\"mnth\"], df[\"weekday\"],          \n","                        values=df[\"total_rentals\"], aggfunc=\"mean\"))\n","\n","# Customize a heatmap\n","\n","sns.heatmap(df_crosstab, annot=True, fmt=\"d\", \n","            cmap=\"YlGnBu\", cbar=False, linewidths=.5, center=df_crosstab.loc[9, 6])\n","# 'annot=True' to turn on annotations in the individual cells.\n","# 'fmt' option ensures that the results are displayed as integers.\n","# 'cmap' of yellow, green and blue to change the shading that we use.\n","# 'cbar=False' the color bar is not displayed.\n","# 'linewidths' puts small spacing between the cells so that the values are simpler to view.\n","# 'center' overall color scheme is shifted towards yellows instead of blues.\n","\n","# Creating a PairGrid\n","\n","g = sns.PairGrid(df, vars=[\"Fair_Mrkt_Rent\",\"Median_Income\"])\n","g = g.map(sns.scatterplot)\n","# we do not define the row and column parameters instead we define the variables\n","\n","# Customazing the PairGrid diagonals\n","\n","g = sns.PairGrid(df, vars=[\"Fair_Mrkt_Rent\", \"Median_Income\"])\n","g = g.map_diag(sns.histplot)\n","g = g.map_offdiag(sns.scatterplot)\n","# 'map_diag()' to define the plotting function for the main diagonal.\n","# 'map_offdiag()' defines the other diagonal.\n","\n","# pairplot() (is a short cut for the PairGrid)\n","\n","sns.pairplot(df, vars=[\"Fair_Mrkt_Rent\",\"Median_Income\"], \n","             kind=\"reg\", diag_kind=\"hist\")\n","\n","# Basic JointGrid\n","\n","g = sns.JointGrid(data=df, x=\"Tuition\", y=\"ADM_RATE_ALL\") \n","g.plot(sns.regplot, sns.histplot)\n","\n","# Advanced JointGrid\n","\n","g = sns.JointGrid(data=df, x=\"Tuition\", y=\"ADM_RATE_ALL\") \n","g = g.plot_joint(sns.kdeplot)\n","g = g.plot_marginals(sns.kdeplot, shade=True)\n","\n","# jointplot() (is a short cut for the JointGrid)\n","\n","sns.jointplot(data=df, x=\"Tuition\", y=\"ADM_RATE_ALL\", kind='hex')\n"]},{"cell_type":"markdown","id":"533d8d3e-9b19-4bcb-be9e-42197e3f740f","metadata":{},"source":["# Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"id":"586d187f-b773-4865-b213-33de56cc26c9","metadata":{},"outputs":[],"source":["# Data summarization\n","\n","# Aggregating ungrouped data\n","books.agg(['mean', 'std'])\n","\n","# Specifuing aggreagations for columns\n","books.agg({'rating': ['mean', 'std'], 'year':['median']})\n","\n","# Named summary columns\n","books.groupby('genre').agg(mean_rating=('rating', 'mean'), \n","                           std_rating=('rating', 'std'), \n","                           median_year=('year', 'median'))"]},{"cell_type":"markdown","id":"ea500538-4487-4bbe-855c-498f05f0cc16","metadata":{},"source":["## Data Cleaning and Imputation\n","### Strategies for addressing missing data\n","Drop missing values\n","- 5% or less of total values\n","Impute mean, median or mode\n","- Depends on distribution and context\n","Impute by sub-groups\n","- e.g. different experience levels have different median salary"]},{"cell_type":"code","execution_count":null,"id":"f0784d96-4c5b-4516-9c41-d06c6754936d","metadata":{},"outputs":[],"source":["# Checking for missing values\n","print(salaries.isana().sum())\n","'''\n","output:\n","Working_Year            12\n","Designation             27\n","Experience              33\n","Employment_Status       31\n","Employee_Location       28\n","Company_Size            40\n","Remote_Working_Ratio    24\n","Salary_USD              60\n","'''\n","# Dropping missing values\n","threshold = len(salaries) * 0.05\n","'''\n","output: 30\n","'''\n","# In this case, we can use Boolean indexing to filter \n","# for columns with missing values less than or equal \n","# to this threshold, storing them as a variable.\n","cols_to_drop = salaries.columns[salaries.isna().sum() <= threshold]\n","'''\n","output: Index(['Working_Year', 'Designation', 'Employee_Location',                 'Remote_Working_Ratio'], dtype='object')\n","'''\n","# now, we need to use the dropna():\n","salaries.dropna(subset=cols_to_drop, inplace=True)\n","# after that we filter the remaining columns with missing values\n","\n","\n","# Checking the remaining columns with missing values\n","cols_with_missing_values = salaries.columns[salaries.isna().sum() > 0]\n","'''\n","Index(['Experience', 'Employment_Status', 'Company_Size', 'Salary_USD'],     dtype='object')\n","'''\n","# Imputing a summary statistic\n","for col in cols_with_missing_values[:-1]:\n","    salaries[col].fillna(salaries[col].mode()[0])\n","\n","\n","# Checking the remaining missing values\n","print(salaries.isna().sum())\n","'''\n","Working_Year             0\n","Designation              0\n","Experience               0\n","Employment_Status        0\n","Employee_Location        0\n","Company_Size             0\n","Remote_Working_Ratio     0\n","Salary_USD              41\n","'''\n","# Calculating the median salary for each group\n","salaries_dict = salaries.groupby('Experience')['Salary_USD'].median().to_dict()\n","'''output: 'Entry': 55380.0, 'Executive': 135439.0, 'Mid': 74173.5, 'Senior': 128903.0}'''\n","\n","# Imputing by sub-groups\n","salaries['Salary_USD'] = salalaries['Salary_USD'].fillna(salaries['Experience'].map(salaries_dict))\n","# we call the map() method because we want to apply the function to ALL elements in the sequence.\n","'''\n","output:\n","Working_Year            0\n","Designation             0\n","Experience              0\n","Employment_Status       0\n","Employee_Location       0\n","Company_Size            0\n","Remote_Working_Ratio    0\n","Salary_USD              0\n","'''"]},{"cell_type":"markdown","id":"585e6711-18f7-4d9d-b561-c183217a98be","metadata":{},"source":["## Converting an analyzing categorical data"]},{"cell_type":"code","execution_count":null,"id":"49a2556c-fa49-4be4-a170-c87bdbdfb2e6","metadata":{},"outputs":[],"source":["# Previewing the data\n","print(salaries.select_dtypes('object').head())\n","print(salaries['Designation'].nunique)\n","# we can count how many job titles there are\n","\n","# Extracting value from categories\n","'''\n","- Current format limits our ability to generate insights\n","- pandas.Series.str.contains()\n","1. Search a column for a specific string or multiple strings\n","'''\n","salaries['Designation'].str.contains('Scientist')\n","\n","# Finding multiple phrases in strings\n","salaries['Designation'].str.contains('Machine Learning|AI')\n","'''\n","we neeed to include a pipe ('|') between our two phrases, notice that we avoid \n","spaces before or after the pipe - if we include them then str.contains will only\n","capture values that have a space\n","'''\n","\n","# Creating the categorical column\n","job_categories = [\"Data Science\", \"Data Analytics\", \"Data Engineering\", \n","                  \"Machine Learning\",\"Managerial\", \"Consultant\"]\n","\n","data_science = \"Data Scientist|NLP\"\n","data_analyst = \"Analyst|Analytics\"\n","data_engineer = \"Data Engineer|ETL|Architect|Infrastructure\"\n","ml_engineer = \"Machine Learning|ML|Big Data|AI\"\n","manager = \"Manager|Head|Director|Lead|Principal|Staff\"\n","consultant = \"Consultant|Freelance\"\n","\n","search_strings = [data_science, data_analyst, data_engineer, ml_engineer, manager, consultant]\n","conditions = []\n","for search_string in search_strings:\n","    conditions.append(salaries[\"Designation\"].str.contains(search_string))\n","# now we can create the categorical column\n","salaries[\"Job_Category\"] = np.select(conditions,\n","                                     job_categories, \n","                                     default=\"Other\")\n"]},{"cell_type":"markdown","id":"5d224c56-8bf0-4088-aa5c-d134ab999d05","metadata":{},"source":["## Working with numeric data\n","- Remove comma values in Salary_In_Rupees\n","- Convert the column to float data type\n","- Create a new column by converting the currency"]},{"cell_type":"code","execution_count":null,"id":"3ee85fd4-97b9-4b06-ba8a-a16f3168f7ab","metadata":{},"outputs":[],"source":["# The original dataset\n","print(salaries[\"Salary_In_Rupees\"].head())\n","'''\n","output:\n","0    20,688,070.00\n","1     8,674,985.00\n","2     1,591,390.00\n","3    11,935,425.00\n","4     5,729,004.00\n","Name: Salary_In_Rupees, dtype: object\n","'''\n","# Converting strings to numbers\n","# pd.Series.str.replace('characters to remove', 'characters to replace them with')\n","salaries['Salary_In_Rupees'] = salaries['Salary_In_rupees'].str.replace(',', '')\n","print(salary['Salary_In_Rupees'].head())\n","'''\n","output:\n","1    20688070.00\n","2     8674985.00\n","3     1591390.00\n","4    11935425.00\n","5     5729004.00\n","Name: Salary_In_Rupees, dtype: object\n","'''\n","# Converting strings to number\n","salaries['Salary_In_Rupees'] = salaries['Salary_In_rupees'].astype(float)\n","# 1 Indian Rupee = 0.012 US Dollars\n","salaries['Salary_USD'] = salaries['Salary_In_rupees'] * 0.012\n","print(salaries['Salary_USD'].head())\n","'''\n","output:\n","0        248256.840\n","1        104099.820\n","2         19096.680\n","3        143225.100\n","4         68748.048\n","'''\n","# Adding summary statistics into a DataFrame\n","'''\n","Group by  --> Select   --> Call     -->  Apply lamba\n","Experience    Salart_USD   transform()    function\n","'''\n","salaries['std_dev'] = salaries.groupby('Experience') \\['Salary_USD'].transform(lambda x: x.std())\n","# we use a backslash ('\\') to split our code over two lines"]},{"cell_type":"markdown","id":"f9a4542e-5b05-4384-a6be-88cecd4bd8bd","metadata":{},"source":["## Hadling outliers\n","### Using the interquartile range (IQR)\n","- IQR = 75th - 25th percentile\n","- Upper Outliers > 75th percentile + (1.5 * IQR)\n","- Lower Outliers < 25th percentile - (1.5 * IQR)"]},{"cell_type":"code","execution_count":null,"id":"a1a7c8c7-8b8d-479f-b80b-774bc5add822","metadata":{},"outputs":[],"source":["# Step by step\n","\n","# First, using descriptive statistcs to find if there's outliers\n","print(salaries['Salary_USD'].describe())\n","\n","# Visualazing them in boxplots\n","sns.boxplot(data=salaries, x='Salary_USD')\n","plt.show()\n","\n","# Identifying thresholds\n","seventy_fifth = salaries['Salary_USD'].quantile(0.75)\n","twent_fifth = salaries['Salary_USD'].quantile(0.25)\n","salaries_iqr = seventy_fifth - twenty_fifth\n","\n","# Identifying outliers\n","upper = seventy_fifth + (1.5 * salaries_iqr)\n","lower = twenty_fifth - (1.5 * salaries_iqr)\n","\n","# Subsetting our data\n","salaries[(salaries[\"Salary_USD\"] < lower) | (salaries[\"Salary_USD\"] > upper)] \\        [[\"Experience\", \"Employee_Location\", \"Salary_USD\"]]\n","\n","# Dropping outliers\n","no_outliers = salaries[(salaries[\"Salary_USD\"] > lower) | (salaries[\"Salary_USD\"] < upper)]\n","# we can remove outliers by modifying the syntax we used to subset our data\n"]},{"cell_type":"markdown","id":"c51d1094-b599-4492-819c-08ca6dcfff4e","metadata":{},"source":["## Relationships in Data"]},{"cell_type":"code","execution_count":null,"id":"f81bca63-cd25-4944-9c5f-03c8815c2a15","metadata":{},"outputs":[],"source":["# Importing DataTime data\n","divorce = pd.read_csv('divorce.csv', parse_dates=['marriage_date'])\n","# With parse_dates argument we can setting it equal to a list of column names that should be interpreted as DataTime data.\n","\n","# Converting to DataTime data (after open the csv file)\n","divorce['marriage_date'] = pd.to_datetime(divorce['marriage_date'])\n","\n","# Creating DataTime data (if the informations was dispersed)\n","divorce['marriage_date'] = pd.to_datatime(divorce[['month', 'day', 'year']])\n","\n","# Extracting parts of a full date (dt.month / dt.day / dt.year)\n","divorce[\"marriage_month\"] = divorce[\"marriage_date\"].dt.month\n"]},{"cell_type":"code","execution_count":null,"id":"47a5b026-04d5-426d-9ca3-214301818506","metadata":{},"outputs":[],"source":["# Correlation heatmaps\n","sns.heatmap(divorce.corr(), annot=True)\n","'''\n","- A heatmap has the benefit of color coding so \n","that strong positive and negative correlations.\n","- The Pearson coefficient \"df.corr()\" only \n","describes the linear correlation between variables.\n","'''\n","# Pairplots\n","sns.pairplot(data=divorce, vars=['income_man', 'income_woman', 'marriage_duration'])\n","'''\n","- It's important to complement our correlation calculations with scatter plots, because they can see non-linear relationships.\n","- Pairplot plots all parwise relationships between numerical variables in one visualization.\n","- But we can limit the number of plotted relationships by setting the 'vars' argument equal to the variables of interest.\n","'''\n","# Kernel Density Estimate (KDE) plots\n","sns.kdeplot(data=divorce, x='marriage_duration', hue='education_man', \n","            cut=0, cumulative=True)\n","'''\n","- Similar to histograms, KDEs allow us to visualize distributions.\n","- KDEs are considered more interpretable, though, especially when multiple distributions are shown.\n","- To improve our plot, we can pass the argument 'cut'; cut tells Seaborn how far past the minimum and maximum data values the curve should go when smoothing is applied.\n","_ If we're interested in the cumulative distribution function, we can set the cumulative keyword argument to True.\n","'''"]},{"cell_type":"markdown","id":"a260849d-440b-4efa-b144-2c9452f2227d","metadata":{},"source":["## Turning Exploratory Analysis into Action\n","### Class imbalance\n","Ex: In a sample of a thousand people, 50 were married, 700 were divorced, 250 were single.\n","- This is an example of class imbalance, where one class occurs more frequently than others.\n","- This can bias results, particularly if this class does not occur more frequently in the population."]},{"cell_type":"code","execution_count":null,"id":"92c0c900-c7d2-4339-b46e-18904e759851","metadata":{},"outputs":[],"source":["# Cross-tabulation (which enables us to examine the frequency of combinations of classes)\n","pd.cross(planes['Source'], planes['Destination'], \n","         values=planes['Price'], aggfunc='median')\n","\n","# Creating categories (We can group numeric data and label them as classes)\n","\n","# Descriptive statistics\n","twenty_fifth = planes[\"Price\"].quantile(0.25)\n","median = planes[\"Price\"].median()\n","seventy_fifth = planes[\"Price\"].quantile(0.75)\n","maximum = planes[\"Price\"].max()\n","\n","# Labels and bins\n","labels = [\"Economy\", \"Premium Economy\", \"Business Class\", \"First Class\"]bins = [0, twenty_fifth, median, seventy_fifth, maximum]\n","\n","# Price categories\n","planes[\"Price_Category\"] = pd.cut(planes[\"Price\"], \n","                                  labels=labels, \n","                                  bins=bins)\n"]},{"cell_type":"markdown","id":"fc66e46f-ef0f-425d-bc69-3f2960a26b5c","metadata":{},"source":["# Working with Categorical Data in Python"]},{"cell_type":"markdown","id":"a7239dc9-0532-4cc8-aeee-d500af470161","metadata":{},"source":["## Differences between Categorical and Numerical data\n","### Categorical\n","- Finite number of groups (or categories).\n","- These categories are usually fixed or known (eye color, hair color, etc.).\n","- Known as qualitative data.\n","\n","### Numerical\n","- Known as quantitative data.\n","- Expressed using a numerical value.\n","- Is usually a measurement (height, weight, IQ, etc.).\n","\n"]},{"cell_type":"markdown","id":"bd83d820-b9c2-49fa-8648-b9e9972f8042","metadata":{},"source":["## There's two types of Categorical data\n","### Ordinal\n","- Categorical variables that have a natural order:\n","- ex: Strongly Disagree / Disagree / Neutral / Agree / Strongly Agree\n","### Nominal\n","- Categorical variables that cannot be placed into a natural order:\n","- ex: Blue / Green / Red / Yellow / Purple"]},{"cell_type":"code","execution_count":null,"id":"b0bd7895-598d-46d0-94a3-414108764748","metadata":{},"outputs":[],"source":["# Setting the type of an object column as a categorical column\n","\n","# Default dtype\n","adult[\"Marital Status\"].dtype\n","'''\n","output: dtype('O')\n","'''\n","# Set as categorical:\n","adult[\"Marital Status\"] = adult[\"Marital Status\"].astype(\"category\")\n","adult[\"Marital Status\"].dtype\n","'''\n","output: CategoricalDtype(categories=[' Divorced', ' Married-AF-spouse',' Married-civ-spouse', ' Married-spouse-absent', ' Never-married',' Separated', ' Widowed'], ordered=False)\n","'''\n","# Creating a categorical Series\n","my_data = [\"A\", \"A\", \"C\", \"B\", \"C\", \"A\"]\n","\n","my_series1 = pd.Series(my_data, dtype=\"category\")\n","print(my_series1)\n","\n","# Another way to create a categorical Series\n","my_data = [\"A\", \"A\", \"C\", \"B\", \"C\", \"A\"]\n","\n","my_series2 = pd.Categorical(my_data, categories=[\"C\", \"B\", \"A\"], ordered=True)\n","print(my_series2)"]},{"cell_type":"code","execution_count":null,"id":"57d5c5c7-4204-4398-8efb-1d00ca41ccc0","metadata":{},"outputs":[],"source":["# Why do we use categorical: memory saver\n","adult = pd.read_csv(\"data/adult.csv\")\n","adult[\"Marital Status\"].nbytes\n","'''\n","output: 260488\n","'''\n","adult[\"Marital Status\"] = adult[\"Marital Status\"].astype(\"category\")\n","adult[\"Marital Status\"].nbytes\n","'''\n","output: 32617\n","'''"]},{"cell_type":"code","execution_count":null,"id":"d86d2b0e-be80-4d51-96eb-62b24c754427","metadata":{},"outputs":[],"source":["# Specifying columns\n","\n","# Option 1: only runs .sum() on two columns\n","adult.groupby(by=[\"Above/Below 50k\"])['Age', 'Education Num'].sum()\n","\n","# Option 2: runs .sum() on all numeric columns and then subsets\n","adult.groupby(by=[\"Above/Below 50k\"]).sum()[['Age', 'Education Num']]"]},{"cell_type":"markdown","id":"329d0898-183a-49b3-829a-a63b092b40e5","metadata":{},"source":["## Categorical pandas Series\n","### Series.cat.method_name\n","Common parameters:\n","- new_categories: a list of categories\n","- inplace: Boolean (whether or not the update should overwrite the Series)\n","- ordered: Boolean (whether or not the categorical is treated as an ordered categorical)"]},{"cell_type":"code","execution_count":null,"id":"900d4785-58a2-4c41-97c4-0be51a0d40e5","metadata":{},"outputs":[],"source":["# Setting Series categories\n","dogs['coat'] = dogs['coat'].cat.set_categories(\n","    new_categories=['short', 'medium', 'long']), \n","    ordered=True\n",") \n","# Adding categories\n","dogs[\"likes_people\"] = dogs[\"likes_people\"].astype(\"category\")\n","dogs[\"likes_people\"] = dogs[\"likes_people\"].cat.add_categories(\n","    new_categories=[\"did not check\", \"could not tell\"]\n",")\n","# Check categories\n","dogs[\"likes_people\"].cat.categories\n","'''\n","output: Index(['no', 'yes', 'did not check', 'could not tell'], dtype='object')\n","'''\n","# Removing categories\n","dogs[\"coat\"] = dogs[\"coat\"].astype(\"category\")\n","dogs[\"coat\"] = dogs[\"coat\"].cat.remove_categories(removals=[\"wirehaired\"])"]},{"cell_type":"markdown","id":"f7e357a8-e8d1-4b05-8732-f2bfe65a848e","metadata":{},"source":["## Updating Categories"]},{"cell_type":"code","execution_count":null,"id":"edfa70c7-b009-457d-affa-3eaa379a6c08","metadata":{},"outputs":[],"source":["# Renaming categories\n","'''\n","Series.cat.rename_categories(new_categories=dict)\n","'''\n","# Make a dictionary:\n","my_changes = {\"Unknown Mix\": \"Unknown\"}\n","\n","# Rename the category:\n","dogs[\"breed\"] = dogs[\"breed\"].cat.rename_categories(my_changes)\n","\n","# Renaming categories with a function\n","dogs['sex'] = dogs['sex'].cat.rename_categories(lambda c: c.title())\n","dogs['sex'].cat.categories\n","'''\n","output: Index(['Female', 'Male'], dtype='object')\n","'''\n","# Collapsing categories setup\n","update_colors = {\n","    \"black and brown\": \"black\",\n","    \"black and tan\": \"black\",\n","    \"black and white\": \"black\",\n","}\n","dogs[\"main_color\"] = dogs[\"color\"].replace(update_colors)\n","# the problem with this method is because the data loses its categorical characteristic. So, we need to covert back to categorical\n","dogs['main_color'] = dogs['main_color'].astype('category')"]},{"cell_type":"markdown","id":"baed6005-abcf-44e4-bfbd-912714825a19","metadata":{},"source":["## Reording Categories"]},{"cell_type":"code","execution_count":null,"id":"a8848075-909a-412f-bc16-3c3ec59f745a","metadata":{},"outputs":[],"source":["# Reording example\n","dogs[\"coat\"].cat.reorder_categories(\n","    new_categories = ['short', 'medium', 'wirehaired', 'long'], \n","    ordered=True, \n","    inplace=True\n",")"]},{"cell_type":"markdown","id":"faa7b9cc-a646-4b81-8c59-fced8101cd8f","metadata":{},"source":["## Cleaning and accessing data\n","### Possible issues with categorical data\n","1) Inconsistent values: \"Ham\", \"ham\", \" Ham\"\n","2) Misspelled values: \"Ham\", \"Hma\"\n","3) Wrong dtype: df['Our Column'].dtype\n"]},{"cell_type":"code","execution_count":null,"id":"c21012a7-b18c-4e91-87a9-73c21fcaf677","metadata":{},"outputs":[],"source":["# Identifying issues\n","'''\n","Series.cat.categories\n","Series.value_counts()\n","'''\n","# Fixing issues> whitespace\n","dogs[\"get_along_cats\"] = dogs[\"get_along_cats\"].str.strip()\n","\n","# Fixing issues: capitalization (.title(), .upper(), .lower())\n","dogs[\"get_along_cats\"] = dogs[\"get_along_cats\"].str.title()\n","\n","# Fixing issues: misspelled words\n","replace_map = {\"Noo\": \"No\"}\n","dogs[\"get_along_cats\"].replace(replace_map, inplace=True)\n","\n","# Using the str accessor object\n","dogs[\"breed\"].str.contains(\"Shepherd\", regex=False)\n","\n","# Accessing data with loc\n","dogs.loc[dogs[\"get_along_cats\"] == \"Yes\", \"size\"]"]},{"cell_type":"markdown","id":"29535da8-51fe-4275-b8a0-5330dacaacf5","metadata":{},"source":["## Pitfalls and Encoding\n","### Using categories can be frustrating\n","- Using the **.str** accessor object to manipulate data converts the Series to an object.\n","- The **.apply()** method outputs a new Series as an object.\n","- The common methods of adding, removing, replacing, or setting categories do not allhandle missing categories the same way.\n","- NumPy functions generally do not work with categorical Series.\n"]},{"cell_type":"code","execution_count":null,"id":"6e5e84c8-138e-4ccd-bd27-ec74931dd967","metadata":{},"outputs":[],"source":["# Huge memory savings\n","used_cars['manufacturer_name'].describe()\n","'''\n","output:\n","count          38531\n","unique            55\n","top       Volkswagen\n","freq            4243\n","Name: manufacturer_name, dtype: object\n","'''\n","print(\"As object: \", used_cars['manufacturer_name'].nbytes)\n","print(\"As category: \", used_cars['manufacturer_name'].astype('category').nbytes)\n","'''\n","output:\n","As object: 308248\n","As category: 38971\n","'''\n","# Little memory savings \n","used_cars['odometer_value'].astype('object').describe()\n","'''\n","output:\n","count      38531\n","unique      6063\n","top       300000\n","freq        1794\n","Name: odometer_value, dtype: int64\n","'''\n","print(f\"As float: {used_cars['odometer_value'].nbytes}\")\n","print(f\"As category: {used_cars['odometer_value'].astype('category').nbytes}\")\n","'''\n","output:\n","As float: 308248\n","As category: 125566\n","'''"]},{"cell_type":"code","execution_count":null,"id":"bb1f6750-dbe7-4fbf-9d9d-82bcd3e09646","metadata":{},"outputs":[],"source":["# Using NumPy arrays\n","# Don't do this\n","used_cars['number_of_photos'] = used_cars['number_of_photos'].astype(\"category\")\n","used_cars['number_of_photos'].sum()  # <--- Gives an Error\n","'''\n","output:\n","TypeError: Categorical cannot perform the operation sum\n","'''\n","# Do this\n","used_cars['number_of_photos'].astype(int).sum()\n","# Note: .str converts the column to an array\n","used_cars[\"color\"].str.contains(\"red\")\n","'''\n","output:\n","0        False\n","1        False\n","...\n","'''"]},{"cell_type":"markdown","id":"43d1bde1-2c08-4857-aee1-faa005fe4bd7","metadata":{},"source":["## Label encoding\n","### What is label enconding?\n","#### The basics:\n","- Codes each category as an integer from 0 through n - 1, where n is the number ofcategories\n","- A -1 code is reserved for any missing values\n","- Can save on memory\n","- Often used in surveys\n","\n","#### The drawback:\n","- Is not the best encoding method for machine learning (see next lesson)\n"]},{"cell_type":"code","execution_count":null,"id":"345bf872-cde7-4be9-b13e-f1c06b2e3960","metadata":{},"outputs":[],"source":["# Creating codes\n","used_cars['manufacturer_name'] = used_cars['manufacturer_name'].astype(\"category\")\n","# Using .cat.codes we can get a label encoding, which will convert the values to integers\n","used_cars['manufacturer_code'] = used_cars['manufacturer_name'].cat.codes\n","\n","# Creating a code book\n","codes = used_cars['manufacturer_name'].cat.codes\n","categories = used_cars['manufacturer_name']\n","\n","name_map = dict(zip(codes, categories))\n","print(name_map)\n","'''\n","output:\n","{45: 'Subaru', \n"," 24: 'LADA', \n"," 12: 'Dodge', \n"," ...}\n","'''"]},{"cell_type":"code","execution_count":null,"id":"7877fc1c-6399-4111-833e-0f4d8e104940","metadata":{},"outputs":[],"source":["# Using a code book\n","\n","# Creating the codes:\n","used_cars['manufacturer_code'] = used_cars['manufacturer_name'].cat.codes\n","\n","# Reverting to previous values:\n","used_cars['manufacturer_code'].map(name_map) # it's similar to .replace(), and it will replace the Series values based on the keys of the 'name_map' and their corresponding values.\n","'''\n","output:\n","0        Acura\n","1        Acura\n","2        Acura\n","...\n","'''"]},{"cell_type":"code","execution_count":null,"id":"15204768-ab18-47d0-abbc-bb5b7d36fc1f","metadata":{},"outputs":[],"source":["# Boolean coding\n","\n","# Find all body types that have \"van\" in them:\n","used_cars[\"body_type\"].str.contains(\"van\", regex=False)\n","\n","# Create a boolean coding:\n","used_cars[\"van_code\"] = np.where(\n","    used_cars[\"body_type\"].str.contains(\"van\", regex=False), 1, 0)\n","# we use the np.where to say anytime this statement is true, we want to have a 1 value, and anytime this statement is false we want to have a 0.\n","used_cars[\"van_code\"].value_counts()\n","'''\n","output:\n","0    34115\n","1     4416\n","Name: van_code, dtype: int64\n","'''"]},{"cell_type":"markdown","id":"90696217-489c-4efa-a0c3-866f3b642ed3","metadata":{},"source":["## One-hot encoding\n","### pd.get_dummies()\n","- data: a pandas DataFrame\n","- columns: a list-like object of column names\n","- prefix: a string to add to the beginning of each category\n","### A few quick notes\n","- Might create too many features\n","- **NaN** values do not get their own column\n"]},{"cell_type":"code","execution_count":null,"id":"fe122a1d-ad8f-436e-b8a5-ee7e46f74c85","metadata":{},"outputs":[],"source":["# One-hot encoding on a DataFrame\n","used_cars_onehot = pd.get_dummies(used_cars[[\"odometer_value\", \"color\"]])\n","\n","# Specifying columns to use\n","used_cars_onehot = pd.get_dummies(used_cars, columns=[\"color\"], prefix=\"\")"]},{"cell_type":"markdown","id":"cb8ae868-c41b-4b60-9ef9-f5adc4b88604","metadata":{},"source":["# Introduction to Importing Data in Python"]},{"cell_type":"markdown","id":"7bcf5dd7-d9c0-4bad-9102-39d1957697df","metadata":{},"source":["## Reading a text file /  Writing to a file / Context manager with"]},{"cell_type":"code","execution_count":null,"id":"9043b4b0-aed2-449f-94eb-2f79535bf151","metadata":{},"outputs":[],"source":["# Reading a text file\n","filename = 'huck_finn.txt'\n","file = open(filename, mode='r') # 'r' is to read\n","text = file.read()\n","file.close()\n","print(text)\n","'''\n","output:\n","YOU don't know about me without you have read a book by the name of The Adventures of Tom Sawyer;\n","but that ain't no matter. That book was made by Mr. Mark Twain, and he told the truth, mainly.\n","There was things which he stretched, but mainly he told the truth.\n","That is nothing. never seen anybody but lied one time or another,\n","without it was Aunt Polly, or the widow, or maybe Mary. Aunt Polly--Tom's Aunt Polly,\n","she is--andMary, and the Widow Douglas is all told about in thatbook, which is mostly a true book,\n","with some stretchers, as I said before.\n","'''"]},{"cell_type":"code","execution_count":null,"id":"b650d718-e082-4bea-846c-277be9cb1bc2","metadata":{},"outputs":[],"source":["# Writing to a file\n","filename = 'huck_finn.txt'\n","file = open(filename, mode='w') # 'w' is to write\n","# we use this if we want to open a file in order to write to it\n","file.close()"]},{"cell_type":"code","execution_count":null,"id":"17cfda7e-84de-4473-9d58-ed02c09291e0","metadata":{},"outputs":[],"source":["# Context manager with\n","# we use this if we want to avoid closing the connection to the file \n","with open('huck_finn.txt', 'r') as file:\n","    print(file.read())\n","# this allows us to create a context in which you can execute commands with the file open"]},{"cell_type":"markdown","id":"66c3b106-e80d-49f7-abd4-8d445e793d99","metadata":{},"source":["## Importing flat files using NumPy"]},{"cell_type":"code","execution_count":null,"id":"d9cf9aaf-3fc0-4f14-a63c-b0e064d19f7c","metadata":{},"outputs":[],"source":["# Customizing your NumPy import\n","import numpy as np\n","filename = 'MNIST_header.txt'\n","data = np.loadtxt(filename, delimiter=',', skiprows=1, usecols=[0, 2])\n","# the default 'delimiter' is any white space so we'll usually nedd to specify it explicitly\n","# if you data consists of numerics and your header has strings in it, you will want to skip the first eow calling the argument 'skiprows = 1'\n","# if you want to set your columns, just use 'usecols=[]'\n","print(data)\n","'''\n","output:\n","[[   0.    0.] \n"," [  86.  254.] \n"," [   0.    0.] \n","...,  \n"," [   0.    0.] \n"," [   0.    0.] \n"," [   0.    0.]]\n","'''"]},{"cell_type":"code","execution_count":null,"id":"738352df-7557-4e37-849c-cdd7eef85cb4","metadata":{},"outputs":[],"source":["# OBS: we can also import different datatypes into NumPy arrays with the 'dtype'\n","data = np.loadtxt(filename, delimiter=',', dtype=str)\n","# 'loadtxt' is great for basic cases, but tends to break down when we have mixed dataypes, for example, the Titanic dataset\n","\n","# Handling with mixed dataypes \n","data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)\n","d = np.recfromcsv(file, delimiter=',', names=True, dtype=None)"]},{"cell_type":"markdown","id":"275aa3ff-962b-4662-8816-656924a60781","metadata":{},"source":["## Importing flat files using pandas\n","### What a data scientist needs:\n","- Two-dimensional labeled data structure(s)\n","- Columns of potentially different types\n","- Manipulate, slice, reshape, groupby, join, merge\n","- Perform statistics\n","- Work with time series data\n"]},{"cell_type":"code","execution_count":null,"id":"2b365594-87d0-4386-acc2-c1b569f3fdfd","metadata":{},"outputs":[],"source":["# Importing using pandas\n","import pandas as pd\n","filename = 'winequality-red.csv'\n","data = pd.read_csv(filename)\n","data.head()"]},{"cell_type":"markdown","id":"99a97eac-d0f4-491c-bf72-f3991c6a9ffb","metadata":{},"source":["## Introduction to other file types\n","### Pickled files\n","- File type native to Python\n","- Motivation: many datatypes datatypes for which it isn't obvious how to store them\n","- Pickled files are serialized\n","- Serialize = covert object to bytestream\n"]},{"cell_type":"code","execution_count":null,"id":"0b9c9e76-d81f-481a-a313-9b1db954ed3c","metadata":{},"outputs":[],"source":["# Pickled files\n","import pickle\n","with open('pickled_fruit.pkl', 'rb') as file:\n","    data = pickle.load(file)    \n","# to specify both read only and binary, you'll want pass the string 'rb' as the second argument of open\n","print(data)\n","'''\n","output: {'peaches': 13, 'apples': 4, 'oranges': 11}\n","'''\n","\n","# Importing Excel spreadsheets\n","import pandas as pd\n","file = 'urbanpop.xlsx'\n","data = pd.ExcelFile(file)\n","print(data.sheet_names)\n","'''\n","output: ['1960-1966', '1967-1974', '1975-2011']\n","'''\n","df1 = data.parse('1960-1966') # sheet name, as a string\n","df2 = data.parse(0) # sheet index, as a float\n"]},{"cell_type":"markdown","id":"ecc7cd90-08b2-4f4a-bbb8-53c3ffa24e75","metadata":{},"source":["### SAS and Stata files\n","- SAS: Statistical Analysis System\n","- Stata: “Statistics” + “data”\n","- SAS: business analytics and biostatistics\n","- Stata: academic social sciences research"]},{"cell_type":"code","execution_count":null,"id":"1ab30f3a-0579-4643-8736-620fb88b5441","metadata":{},"outputs":[],"source":["# Importing SAS files\n","import pandas as pd\n","from sas7bdat import SAS7BDAT\n","with SAS7BDAT('urbanpop.sas7bdat') as file:    \n","    df_sas = file.to_data_frame()\n","\n","# Importing Stata files\n","import pandas as pd\n","data = pd.read_stata('urbanpop.dta')\n"]},{"cell_type":"markdown","id":"012eef20-8a2e-4653-bccd-60ebfe5255a2","metadata":{},"source":["### HDF5 files\n","- Hierarchical Data Format version 5\n","- Standard for storing large quantities of numerical data\n","- Datasets can be hundreds of gigabytes or terabytes\n","- HDF5 can scale to exabytes"]},{"cell_type":"code","execution_count":null,"id":"32a95338-b9a5-42eb-9c84-9ccbe8418907","metadata":{},"outputs":[],"source":["# Importing HDF5 files\n","import h5py\n","filename = 'H-H1_LOSC_4_V1-815411200-4096.hdf5'\n","data = h5py.File(filename, 'r') # 'r' is to read"]},{"cell_type":"markdown","id":"025ad39b-c2d4-49d1-a9ae-d2093d5d6cca","metadata":{},"source":["### MATLAB files\n","- “Matrix Laboratory”\n","- Industry standard in engineering and science\n","- Data saved as .mat files\n","\n","\n","### SciPy to the rescue!\n","- scipy.io.loadmat() - read .mat files\n","- scipy.io.savemat() - write .mat files\n"]},{"cell_type":"code","execution_count":null,"id":"158b5602-7b9c-4c44-afa9-5a4f1792c9de","metadata":{},"outputs":[],"source":["# Importing a .mat file\n","import scipy.io\n","filename = 'workspace.mat'\n","mat = scipy.io.loadmat(filename)"]},{"cell_type":"markdown","id":"7bcf52a1-8485-4c81-aa3d-ee6c774a5c97","metadata":{},"source":["## Working with relational databases"]},{"cell_type":"code","execution_count":null,"id":"482c1029-7942-4883-93f9-601dab8a0f35","metadata":{},"outputs":[],"source":["# Creating a database engine\n","from sqlalchemy import create_engine\n","engine = create_engine('sqlite:///Northwind.sqlite')\n","\n","# Getting table names\n","table_names = engine.table_names()\n","print(table_names)\n","'''\n","output:\n","['Categories', 'Customers', 'EmployeeTerritories',\n","'Employees', 'Order Details', 'Orders', 'Products',\n","'Region', 'Shippers', 'Suppliers', 'Territories']\n","'''"]},{"cell_type":"code","execution_count":null,"id":"d486410c-7d1b-4aef-b8fd-68206f50e90d","metadata":{},"outputs":[],"source":["# Querying relational databases\n","from sqlalchemy import create_engine\n","import pandas as pd\n","\n","engine = create_engine('sqlite:///Northwind.sqlite')\n","con = engine.connect()\n","rs = con.execute(\"SELECT * FROM Orders\")\n","df = pd.DataFrame(rs.fetchall())\n","df.columns = rs.keys()\n","\n","con.close()\n","\n","# Using the context manager\n","from sqlalchemy import create_engine\n","import pandas as pd\n","\n","engine = create_engine('sqlite:///Northwind.sqlite')\n","\n","with engine.connect() as con:    \n","    rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")    \n","    df = pd.DataFrame(rs.fetchmany(size=5))    \n","    df.columns = rs.keys()\n","# we have another way to do that in just one line\n","df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n","\n","# More functionalities\n","from sqlalchemy import create_engine\n","import pandas as pd\n","\n","engine = create_engine('sqlite:///Chinook.sqlite')\n","\n","df = pd.read_sql_query(\"SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate\", engine)"]},{"cell_type":"code","execution_count":null,"id":"7bf542a2-b0c7-4353-abd5-e28b602d84ed","metadata":{},"outputs":[],"source":["# JOINg tables\n","from sqlalchemy import create_engine\n","import pandas as pd\n","\n","engine = create_engine('sqlite:///Northwind.sqlite')\n","\n","df = pd.read_sql_query(\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\", engine)\n","\n","print(df.head())"]},{"cell_type":"markdown","id":"1d3ddf72","metadata":{},"source":["# Intermediate Importing Data in Python"]},{"cell_type":"markdown","id":"fd856df7","metadata":{},"source":["## Importing data from the internet"]},{"cell_type":"code","execution_count":null,"id":"980af959","metadata":{},"outputs":[],"source":["# Automate file download in Python\n","from urllib.request import urlretrieve\n","import pandas as pd\n","url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n","\n","urlretrieve(url, 'winequality-red.csv')\n","\n","df = pd.read_csv('winequality-red.csv', sep=';')\n","print(df.head())\n","\n","# Read in all sheets of Excel file\n","url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n","xls = pd.read_excel(url, sheet_name=None)\n","print(xls.keys())\n","print(xls['1700'].head())"]},{"cell_type":"code","execution_count":null,"id":"c789e402","metadata":{},"outputs":[],"source":["# Import flat files from the web\n","from urllib.request import urlopen, Request\n","url = \"https://www.wikipedia.org/\"\n","request = Request(url)\n","response = urlopen(request)\n","html = response.read()\n","response.close()\n","\n","# GET requests using requests\n","import requests\n","surl = \"https://www.wikipedia.org/\"\n","r = requests.get(url)\n","text = r.text\n"]},{"cell_type":"markdown","id":"3cb15a57","metadata":{},"source":["### HTML\n","Mix of unstructured and structured data\n","\n","Structured data:\n","- Has pre-defined data model, or\n","- Organized in a difined manner\n","\n","Unstructured data:\n","- Neither of these properties"]},{"cell_type":"code","execution_count":null,"id":"465cbeb9","metadata":{},"outputs":[],"source":["# BeautifulSoup\n","from bs4 import BeautifulSoup\n","import requests\n","url = 'https://www.crummy.com/software/BeautifulSoup/'\n","r = requests.get(url)\n","html_doc = r.text\n","soup = BeautifulSoup(html_doc)\n","print(soup.prettify())"]},{"cell_type":"code","execution_count":null,"id":"801c284b","metadata":{},"outputs":[],"source":["# Exploring BeautifulSoup\n","print(soup.title)\n","\n","print(soup.get_text())\n","\n","for link in soup.find_all('a'):\n","    print(link.get('href'))"]},{"cell_type":"markdown","id":"f9728311","metadata":{},"source":["## Introduction do APIs and JSONs\n","\n","### API\n","1. Application Programming Interface\n","2. Set of protocols and routines\n","- Building and interacting with software applications\n","3. Bunch of code\n","- Allows two software programs to communicate with each other\n","\n","Obs: A standard form for transfering data through APIs is the JSON file format\n","\n","### JSON\n","- JavaScript Object Notation\n","- Real-time server-to-browser communication\n","- Human readable"]},{"cell_type":"code","execution_count":null,"id":"a5c79434","metadata":{},"outputs":[],"source":["# Loading JSONs in Python\n","import json\n","with open('snakes.json', 'r') as json_file:\n","    json_data = json.load(json_file)\n","    \n","for key, value in json_data.items():\n","    print(f\"{key}:{value}\")"]},{"cell_type":"code","execution_count":null,"id":"69098d43","metadata":{},"outputs":[],"source":["# Connecting to an API in Python\n","import requests\n","url = 'http://www.omdbapi.com/?t=hackers'\n","r = requests.get(url)\n","json_data = r.json()\n","\n","for key, value in json_data.items():\n","    print(f'{key}: {value}')"]},{"cell_type":"markdown","id":"2321db05","metadata":{},"source":["## Using Tweepy: Authentication"]},{"cell_type":"code","execution_count":null,"id":"fd5079c0","metadata":{},"outputs":[],"source":["# Using Tweepy: Authentication handler\n","import tweepy, json\n","access_token = \"...\"\n","access_token_secret = \"...\"\n","consumer_key = \"...\"\n","consumer_secret = \"...\"\n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_token, access_token_secret)"]},{"cell_type":"code","execution_count":null,"id":"f44eb804","metadata":{},"outputs":[],"source":["# Tweepy: define stream listener class\n","class MyStreamListener(tweepy.StreamListener):\n","    def __init__(self, api=None):\n","        super(MyStreamListener, self).__init__()\n","        self.num_tweets = 0\n","        self.file = open(\"tweets.txt\", \"w\")\n","    def on_status(self, status):\n","        tweet = status._json\n","        self.file.write(json.dumps(tweet) + '\\\\n')\n","        tweet_list.append(status)\n","        self.num_tweets += 1\n","        if self.num_tweets < 100:\n","            return True \n","        else: \n","            return False\n","        self.file.close()"]},{"cell_type":"code","execution_count":null,"id":"44cc64f2","metadata":{},"outputs":[],"source":["# Using Tweepy: stream tweets!\n","\n","# Create Streaming object and authenticate\n","l = MyStreamListener() \n","stream = tweepy.Stream(auth, l)\n","\n","# This line filters Twitter Streams to capture data by keywords:\n","stream.filter(track=['apples', 'oranges'])"]},{"cell_type":"code","execution_count":null,"id":"1b166c0f","metadata":{},"outputs":[],"source":["# Create Streaming object\n","stream = tweepy.Stream(consumer_key, consumer_secret, \n","                       access_token, access_token_secret)\n"]},{"cell_type":"markdown","id":"2fb875c4-0755-4e9d-a68c-a6ee556a2b9b","metadata":{},"source":["# Cleaning Data in Python "]},{"cell_type":"markdown","id":"cd8a1329-347c-4d97-8b76-3883b16dcfd2","metadata":{},"source":["### Data type constraints"]},{"cell_type":"code","execution_count":null,"id":"4e9ad4dc-e57d-4422-8e23-d29027b92ca7","metadata":{},"outputs":[],"source":["# String to integers\n","sales.dtypes\n","'''\n","output:\n","lesOrderID      int64\n","Revenue         object\n","Quantity        int64\n","'''\n","# Print sum of all Revenue column\n","sales['Revenue'].sum()\n","'''\n","output:\n","'23153$1457$36865$32474$472$27510$16158$5694$6876$40487$807$6893$9153$6895$4216..\n","'''\n","# Remove $ from Revenue column\n","sales['Revenue'] = sales['Revenue'].str.strip('$')\n","sales['Revenue'] = sales['Revenue'].astype('int')\n","\n","# Verify that Revenue is now an integer\n","assert sales['Revenue'].dtype == 'int'"]},{"cell_type":"code","execution_count":null,"id":"c53bd66f-8ba4-489d-8437-b3e043737978","metadata":{"collapsed":true,"executionTime":594,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastSuccessfullyExecutedCode":"# The assert statement\n# This will pass\nassert 1+1 == 2\n\n# This will not pass\nassert 1+1 == 3"},"outputs":[{"ename":"AssertionError","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This will not pass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["# The assert statement\n","# This will pass\n","assert 1 + 1 == 2\n","\n","# This will not pass (error)\n","assert 1 + 1 == 3"]},{"cell_type":"markdown","id":"9049530f-dd32-4771-89e6-828b93a2fcb8","metadata":{},"source":["## Data range constraints"]},{"cell_type":"code","execution_count":null,"id":"cc7993a9-3691-47dc-867a-52ad3ed7fa3b","metadata":{},"outputs":[],"source":["# Convert avg_rating > 5 to 5\n","movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5\n","\n","# Assert statement\n","assert movies['avg_rating'].max() <= 5"]},{"cell_type":"code","execution_count":null,"id":"37da3032-553a-4b32-82e3-3b2fd1836608","metadata":{},"outputs":[],"source":["# Date range example\n","import datetime as dt\n","import pandas as pd\n","# Output data types\n","user_signups.dtypes\n","'''\n","output:\n","subscription_date    object\n","user_name            object\n","Country              object\n","dtype: object\n","'''\n","# Convert to date\n","user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date\n"]},{"cell_type":"code","execution_count":null,"id":"71748683-2a15-494e-ad5f-ebbc6e4bace3","metadata":{},"outputs":[],"source":["# Date range example\n","today_date = dt.date.today()\n","\n","# Drop the data \n","# Drop values using filtering\n","user_signups = user_signups[user_signups['subscription_date'] < today_date]\n","# Drop values using .drop()\n","user_signups.drop(user_signups[user_signups['subscription_date'] > today_date].index, inplace = True)\n","\n","# Hardcode dates with upper limit \n","# Drop values using filtering\n","user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n","# Assert is true\n","assert user_signups.subscription_date.max().date() <= today_date"]},{"cell_type":"markdown","id":"2cfbecd9-c2b3-4cbd-b44b-d5e13e27adc2","metadata":{},"source":["## Uniqueness constraints\n","The **.duplicated()** method\n","- **subset**: List of column names to check for duplication. \n","- **keep**: Whether to keep first('first'), last('last') or all(False) duplicate values."]},{"cell_type":"code","execution_count":null,"id":"a6d29708-68de-41ae-9fa2-5062d0f765cc","metadata":{},"outputs":[],"source":["# How to find duplicate values?\n","\n","# Column names to check for duplication\n","column_names = ['first_name','last_name','address']\n","duplicates = height_weight.duplicated(subset = column_names, keep = False)\n","\n","# Get duplicates across all columns\n","duplicates = height_weight.duplicated()\n","\n","# Get duplicate rows\n","duplicates = height_weight.duplicated()\n","height_weight[duplicates]"]},{"cell_type":"markdown","id":"4d685c74-95e4-4d00-8b27-f7181cdc8075","metadata":{},"source":["The **.drop_duplicates()** method\n","- **subset**: List of column names to check for duplication. \n","- **keep**: Whether to keep first('first'), last('last') or all(False) duplicate values.\n","- **inplace**: Drop duplicated rows directly inside DataFrame without creating new object(True)."]},{"cell_type":"code","execution_count":null,"id":"cb67d5a4-74e0-46ab-9fc3-65d6cdb2f707","metadata":{},"outputs":[],"source":["# How to find duplicate rows?\n","\n","# Output duplicate values\n","height_weight[duplicates].sort_values(by = 'first_name')\n","\n","# Drop duplicates\n","height_weight.drop_duplicates(inplace = True)"]},{"cell_type":"code","execution_count":null,"id":"12cebd80-d262-44cc-979a-d47f38b1bc83","metadata":{},"outputs":[],"source":["# How to treat duplicate values?\n","\n","# Group by column names and produce statistical summarie\n","column_names = ['first_name','last_name','address']\n","summaries = {'height': 'max', 'weight': 'mean'}\n","height_weight = height_weight.groupby(column_names).agg(summaries).reset_index()\n","\n","# Make sure aggregation is done\n","duplicates = height_weight.duplicated(subset = column_names, keep = False)\n","height_weight[duplicates].sort_values(by = 'first_name')"]},{"cell_type":"markdown","id":"2f74a806-5b4d-402c-939b-9de46f5435ca","metadata":{},"source":["## Text and categorical data problems\n","- Categorical data represent variables that represent predefined finite set of categories\n","\n","### How do we treat these problems?\n","- Dropping data with incorrect categories\n","- Remapping incorrect categories to correct ones \n","- Inferring categories"]},{"cell_type":"code","execution_count":null,"id":"6023138b-fdc6-46c0-a81b-c3bcc6cf673e","metadata":{},"outputs":[],"source":["# Finding incosistent categories\n","'''\n","The problem is: we some impossible blood types in our dataframe as Z+,\n","so we need to clean it and find the best way to solve this\n","'''\n","inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n","print(inconsistent_categories)\n","'''\n","output: {'Z+'}\n","'''\n","# Get and print rows with inconsistent categories\n","inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n","study_data[inconsistent_rows]      \n","'''\n","output: name   birthday blood_type\n","    5 Jennifer 2019-12-17        Z+\n","''''"]},{"cell_type":"markdown","id":"d2fc5952-44af-4c24-85a6-170a17c24bab","metadata":{},"source":["## What type of errors could we have?\n","### I) Value inconsistency \n","- Inconsistentelds: **'married'**, **'Maried'**, **'UNMARRIED'**, **'not married'**... \n","- _Trailing white spaces: _**'married '**, **' married '**..\n","\n","### II) Collapsing too many categories to few \n","- Creating new groups: **0-20K**, **20-40K** categories... from continuous household income data\n","- Mapping groups to new ones: Mapping household income categories to 2 **'rich'**, **'poor'** \n","\n","### III) Making sure data is of type category\n"]},{"cell_type":"code","execution_count":null,"id":"5a7fcb83-b2a1-47c6-a289-212ce45b4786","metadata":{},"outputs":[],"source":["# Value consistency\n","\n","# Capitalization: 'married', 'Married', 'UNMARRIED', 'unmarried'.\n","# Get marriage status column\n","marriage_status = demographics['marriage_status']\n","marriage_status.value_counts()\n","'''\n","output:\n","unmarried    352\n","married      268\n","MARRIED      204\n","UNMARRIED    176\n","dtype: int64\n","'''\n","# Capitalize\n","marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()\n","marriage_status['marriage_status'].value_counts()\n","# Lowercase\n","marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()\n","marriage_status['marriage_status'].value_counts()\n","\n","\n","# Trailing spaces: 'married ', 'married', 'unmarried', ' unmarried'.\n","# Get marriage status column\n","marriage_status = demographics['marriage_status']\n","marriage_status.value_counts() \n","'''\n","output:\n"," unmarried   352\n","unmarried    268\n","married      204\n","married      176\n","dtype: int64\n","'''\n","# Strip all spaces\n","demographics = demographics['marriage_status'].str.strip()\n","demographics['marriage_status'].value_counts()\n","'''\n","output:\n","unmarried    528\n","married      472\n","'''"]},{"cell_type":"code","execution_count":null,"id":"aa030988-c0e5-4b43-9fa0-fc4be00b7e9e","metadata":{},"outputs":[],"source":["# Collapsing data into categories\n","\n","# Create categories out of data: income_group column from income column\n","\n","# Using qcut()\n","import pandas as pd\n","group_names = ['0-200K', '200K-500K', '500K+']\n","demographics['income_group'] = pd.qcut(demographics['household_income'], q = 3, labels = group_names)\n","\n","# Print income_group column\n","demographics[['income_group', 'household_income']]\n","\n","\n","# Using cut() - create category ranges and names\n","ranges = [0, 200000, 500000, np.inf]\n","group_names = ['0-200K', '200K-500K', '500K+']\n","\n","# Create income group column\n","demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n","\n","# Print income_group column\n","demographics[['income_group', 'household_income']]\n","\n","\n","# Map categories to fewer ones: reducing categories incategorical column.\n","'''\n","operating_system column is: 'Microsoft', 'MacOS', 'IOS', 'Android', 'Linux'\n","operating_system column should become: 'DesktopOS', 'MobileOS'\n","'''\n","# Create mapping dictionary and replace\n","mapping = {'Microsoft':'DesktopOS', 'MacOS':'DesktopOS', 'Linux':'DesktopOS','IOS':'MobileOS', 'Android':'MobileOS'}\n","devices['operating_system'] = devices['operating_system'].replace(mapping)\n","devices['operating_system'].unique()"]},{"cell_type":"markdown","id":"a66982c5-bc1d-42a3-903a-19a042b3d910","metadata":{},"source":["## Cleaning text data\n","### Common text data problemns\n","1. Data inconsistency: +96171679912 or 0096171679912 or..?\n","2. Fixed length violations: Passwords needs to be at least 8 characters \n","3. Typos: +961.71.679912"]},{"cell_type":"code","execution_count":null,"id":"db98fe4e-120d-46e5-b7e5-20dbfb84f09c","metadata":{},"outputs":[],"source":["# Fixing the phone number column\n","\n","# Replace \"+\" with \"00\"\n","phones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"+\", \"00\")\n","\n","# Replace \"-\" with nothing\n","phones[\"Phone number\"] = phones[\"Phone number\"].str.replace(\"-\", \"\")\n","\n","# Replace phone numbers with lower than 10 digits to NaN\n","digits = phones['Phone number'].str.len()\n","phones.loc[digits < 10, \"Phone number\"] = np.nan\n","\n","\n","# Find length of each row in Phone number column\n","sanity_check = phone['Phone number'].str.len()\n","\n","# Assert minmum phone number length is 10\n","assert sanity_check.min() >= 10\n","\n","# Assert all numbers do not have \"+\" or \"-\"\n","assert phone['Phone number'].str.contains(\"+|-\").any() == False"]},{"cell_type":"code","execution_count":null,"id":"75a42a82-583f-4a18-988a-8453e98ab75e","metadata":{},"outputs":[],"source":["# Regular expressions in action\n","\n","# Replace letters with nothing\n","phones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '')\n","phones.head()"]},{"cell_type":"markdown","id":"81b89d26-dbcd-4150-a679-58eff304d52b","metadata":{},"source":["## Uniformity, Cross field validation, Completeness"]},{"cell_type":"markdown","id":"67b216bb-901e-4fe2-9181-75c14285ff90","metadata":{},"source":["### Treating ambiguous date data \n","**Is 2019-03-08 in August or March?**\n","- Convert to NA and treat accordingly\n","- Infer format by understanding data source\n","- Infer format by understanding previous and subsequent data in DataFrame\n","\n","\n","### Cross field validation\n","**What to do when we catch inconsistencies?**\n","- Dropping Data\n","- Set to missing and impute\n","- Apply rules from domain knowledge\n","\n","### Completeness\n","**How to deal with missing data?**\n","**Simple approaches:**\n","1. Drop missing data\n","2. Impute with statistical measures (mean, median, mode...)\n","**More complex approaches:**\n","1. Imputing using an algorithmic approach\n","2. Impute with machine learning models\n","\n","**Missingness types**\n","1. Missing Completely at Random (MCAR):\n","- No systematic relationship between missing data and other values\n","- Data entry erros when imputting data\n","2. Missing at Random (MAR):\n","- Systematic relationship between missing data and other **observed** values\n","- Missing ozone data for high temperatures\n","3. Missing Not at Random (MNAR)\n","- Systematic relationship between missing data and **unobserved** values\n","- Missing temperature values for high temperatures"]},{"cell_type":"code","execution_count":null,"id":"cc513903-7082-4057-a732-8c4323a4c44a","metadata":{},"outputs":[],"source":["# Uniformity\n","\n","# Treating temperature data\n","temp_fah = temperatures.loc[temperatures['Temperature'] > 40, 'Temperature']\n","temp_cels = (temp_fah - 32) * (5/9)\n","temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels\n","\n","# Assert conversion is correct\n","assert temperatures['Temperature'].max() < 4095\n","\n","\n","# Treating date data\n","birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'],\n","                        # Attempt to infer format of each date\n","                        infer_datetime_format=True, \n","                        # Return NA for rows where conversion failed\n","                        errors = 'coerce')\n","\n","# Another way to treating date data\n","birthdays['Birthday'] = birthdays['Birthday'].dt.strftime(\"%d-%m-%Y\")"]},{"cell_type":"code","execution_count":null,"id":"731400da-ebdb-4d0b-a9f6-fe5245bd3309","metadata":{},"outputs":[],"source":["# Cross field validation\n","\n","# Checking data integrity\n","fund_class=['economy_class', 'business_class', 'first_class']\n","sum_classes = flights[fund_class].sum(axis = 1)\n","passenger_equ = sum_classes == flights['total_passengers']\n","\n","# Find and filter out rows with inconsistent passenger totals\n","inconsistent_pass = flights[~passenger_equ]\n","consistent_pass = flights[passenger_equ]\n","\n","\n","import pandas as pd\n","import datetime as dt\n","# Convert to datetime and get today's date\n","users['Birthday'] = pd.to_datetime(users['Birthday'])\n","today = dt.date.today()\n","\n","# For each row in the Birthday column, calculate year difference\n","age_manual = today.year - users['Birthday'].dt.year\n","\n","# Find instances where ages match\n","age_equ = age_manual == users['Age']\n","\n","# Find and filter out rows with inconsistent age\n","inconsistent_age = users[~age_equ]\n","consistent_age = users[age_equ]"]},{"cell_type":"code","execution_count":null,"id":"3a0066ca-09ef-469c-8768-9d3717b438c7","metadata":{},"outputs":[],"source":["# Completeness\n","\n","# Visualizing and understanding missing data\n","import missingno as msno\n","import matplotlib.pyplot as plt\n","\n","# Visualize missingness\n","msno.matrix(airquality)\n","plt.show\n","\n","\n","# Dealing with missing data\n","\n","# Dropping missing values\n","airquality_dropped = airquality.dropna(subset = ['CO2'])\n","\n","# Replacing with statiscal measures\n","co2_mean = airquality['CO2'].mean()\n","airquality_imputed = airquality.fillna({'CO2': co2_mean})"]},{"cell_type":"markdown","id":"1904cb52-6d75-449b-ad39-9aa27427a0fd","metadata":{},"source":["## Record linkage"]},{"cell_type":"markdown","id":"d48bb110-c8d2-4d77-b47c-8ebcfbaaab0e","metadata":{},"source":["### Comparing strings\n","**Minimum edit distance**\n","- Least possible amount of steps needed to transition from one string to another"]},{"cell_type":"code","execution_count":null,"id":"d2853347-d49a-44d7-a9b9-fa03eb582d2c","metadata":{},"outputs":[],"source":["# Simple string comparison\n","\n","# Lets us compare between two strings\n","from thefuzz import fuzz\n","\n","# Compare reeding vs reading\n","fuzz.WRatio('Reeding', 'Reading')"]},{"cell_type":"code","execution_count":null,"id":"dc7c2380-9baf-42f8-8f9f-4d4bf2fa3ceb","metadata":{},"outputs":[],"source":["# Partial strings and different orderings\n","\n","# Partial string comparison\n","fuzz.WRatio('Houston Rockets', 'Rockets')\n","\n","# Partial string comparison with different order\n","fuzz.WRatio('Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')"]},{"cell_type":"code","execution_count":null,"id":"a6445fd5-b4fb-4808-952b-f4fa1d91a80f","metadata":{},"outputs":[],"source":["# Comparison with arrays\n","\n","# Import process\n","from thefuzz import process\n","\n","#Define string and array of possible matches\n","string = \"Houston Rockets vs Los Angeles Lakers\"\n","choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets', 'Houson vs Los                           Angeles', 'Heat vs Bulls'])\n","process.extract(string, choices, limit = 2)"]},{"cell_type":"code","execution_count":null,"id":"0871d3af-a826-4c9f-9f2d-7c6112f48818","metadata":{},"outputs":[],"source":["# Collapsing all of the state\n","\n","# For each correct category\n","for state in categories['state']:\n","    # Find potential matches in states with typoes    \n","    matches = process.extract(state, survey['state'], limit = survey.shape[0])\n","    # For each potential match match\n","    for potential_match in matches:\n","        # If high similarity score\n","        if potential_match[1] >= 80:\n","            # Replace typo with correct category          \n","            survey.loc[survey['state'] == potential_match[0], 'state'] = state"]},{"cell_type":"markdown","id":"2a7b3203-9e39-4852-9fae-2ff7c7564faf","metadata":{},"source":["## Generating pairs"]},{"cell_type":"code","execution_count":null,"id":"8138cad8-3968-433e-8db6-18ea3d2881cb","metadata":{},"outputs":[],"source":["# Generating Pairs\n","\n","# Import recordlinkage\n","import recordlinkage\n","\n","# Create indexing object\n","indexer = recordlinkage.Index()\n","\n","# Generate pairs blocked on state\n","indexer.block('state')\n","pairs = indexer.index(census_A, census_B)"]},{"cell_type":"code","execution_count":null,"id":"ed9a3833-e6b4-4c7b-9588-961b2e9cdbdb","metadata":{},"outputs":[],"source":["# Comparing the DataFrames\n","\n","# Generate the pairs\n","pairs = indexer.index(census_A, census_B)\n","\n","# Create a Compare object\n","compare_cl = recordlinkage.Compare()\n","\n","# Find exact matches for pairs of date_of_birth and state\n","compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n","compare_cl.exact('state', 'state', label='state')\n","\n","# Find similar matches for pairs of surname and address_1 using string similarity\n","compare_cl.string('surname', 'surname', threshold=0.85, label='surname')\n","compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n","\n","# Find matches\n","potential_matches = compare_cl.compute(pairs, census_A, census_B)"]},{"cell_type":"markdown","id":"38fa4f62-3565-40a3-99ff-ca6c53a70cd7","metadata":{},"source":["## Linking DataFrames"]},{"cell_type":"code","execution_count":null,"id":"60927cf6-b86e-4854-819f-6ad0dcab84c3","metadata":{},"outputs":[],"source":["# Propable matches\n","matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n","\n","# Get the indices\n","matches.index\n","'''\n","output:\n","MultiIndex(levels=[['rec-1007-org', 'rec-1016-org', 'rec-1054-org', 'rec-1066-org', 'rec-1070-org', 'rec-1075-org', 'rec-1080-org', 'rec-110-org', ...\n","'''\n","# Get indices from census_B only\n","duplicate_rows = matches.index.get_level_values(1)\n","print(census_B_index)\n","'''\n","output:\n","Index(['rec-2404-dup-0', 'rec-4178-dup-0', 'rec-1054-dup-0', 'rec-4663-dup-0',       'rec-485-dup-0', 'rec-2950-dup-0', 'rec-1234-dup-0', ... , 'rec-299-dup-0'])\n","'''\n","\n","# Linking DataFrames\n","# Finding duplicates in census_B\n","census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n","\n","# Finding new rows in census_B\n","census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n","\n","# Link the DataFrames!\n","full_census = census_A.append(census_B_new)\n","\n","\n","# Import recordlinkage and generate pairs and compare across columns...\n","\n","# Generate potential matches\n","potential_matches = compare_cl.compute(full_pairs, census_A, census_B)\n","\n","# Isolate matches with matching values for 3 or more columns\n","matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n","\n","# Get index for matching census_B rows only\n","duplicate_rows = matches.index.get_level_values(1)\n","\n","# Finding new rows in census_B\n","census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n","\n","# Link the DataFrames!\n","full_census = census_A.append(census_B_new)\n"]},{"cell_type":"markdown","id":"69661cb7-ad76-4b17-a77b-0ede93c13a1a","metadata":{},"source":["# Working with Dates and Times in Python"]},{"cell_type":"markdown","id":"dc9d4436-ea6b-4fb3-b1a6-90104ca64231","metadata":{},"source":["### Weekdays in Python\n","- 0 = Monday\n","- 1 = Tuesday\n","- 2 = Wednesday\n","- 3 = Thursdat\n","- 4 = Friday\n","- 5 = Saturday\n","- 6 = Sunday"]},{"cell_type":"code","execution_count":null,"id":"9721a138-4974-49df-bd46-008c956d5180","metadata":{},"outputs":[],"source":["# Dates in Python\n","\n","# Import date\n","from datetime import date\n","# Create dates\n","two_hurricanes_dates = [date(2016, 10, 7), date(2017, 6, 21)]\n","\n","# Attributes of a date\n","print(two_hurricanes_dates[0].year)\n","print(two_hurricanes_dates[0].month)\n","print(two_hurricanes_dates[0].day)\n","\n","print(two_hurricanes_dates[0].weekday())"]},{"cell_type":"code","execution_count":null,"id":"633f0692-efdf-411f-be52-9aefeb98f314","metadata":{},"outputs":[],"source":["# Turning dates into strings\n","\n","from datetime import date\n","\n","d = date(2017, 11, 5) # Example date\n","# ISO format: YYYY-MM-DD\n","print(d)\n","\n","# Express the date in ISO 8601 format and put it in a list\n","print([d.isoformat()])\n","\n","# A few dates that computers once had trouble with\n","some_dates = ['2000-01-01', '1999-12-31']\n","# Print them in order\n","print(sorted(some_dates))['1999-12-31', '2000-01-01']\n"]},{"cell_type":"code","execution_count":null,"id":"ad910466-f58e-4488-93c4-7c923e5475d9","metadata":{},"outputs":[],"source":["# Other forms to create a date\n","\n","d = date(2017, 1, 5)\n","\n","print(d.strftime(\"Year is %Y\")) # fills the year in this string"]},{"cell_type":"markdown","id":"4946e41d-3b2b-4e45-ab50-1b7f557b8539","metadata":{},"source":["### Dates and Times\n","How to represent this in python:\n","October 1 2017, 3:23:25 PM"]},{"cell_type":"code","execution_count":null,"id":"8c0b3805-3eba-4196-bc07-c32873f2530c","metadata":{},"outputs":[],"source":["# Import datetime\n","from datetime import datetime\n","dt = datetime(2017, 10, 1, 15, 23, 25)\n","# or being more explicit \n","dt = datetime(year=2017, month=10, day=1, \n","              hour=15, minute=23 second=25, \n","              microsecond=500000)\n","print(dt)\n","\n","# Replacing parts of a datetime\n","dt_hr = dt.replace(minute=0, second=0, microsecond=0)\n","print(dt_hr)\n","\n","# Printing datae times\n","print(dt.strftime(\"%Y-%m-%d %H:%M:%S\"))\n","\n","# A timestamp\n","ts = 1514665153.0\n","# Convert to datetime and print\n","print(datetime.fromtimestamp(ts))"]},{"cell_type":"markdown","id":"175ace93-d5bc-4018-9df4-5af65dc6ee03","metadata":{},"source":["### Time Zones and Daylight Saving"]},{"cell_type":"code","execution_count":null,"id":"4028dde9-2fd9-4f1b-ad43-18d27350d505","metadata":{},"outputs":[],"source":["# UTC\n","\n","# Import relevant classes\n","from datetime import datetime, timedelta, timezone\n","# US Eastern Standard time zone\n","ET = timezone(timedelta(hours=-5))\n","# Timezone-aware datetime\n","dt = datetime(2017, 12, 30, 15, 9, 3, tzinfo = ET)\n","print(dt)\n","\n","\n","# India Standard time zone\n","IST = timezone(timedelta(hours=5, minutes=30))\n","# Convert to IST\n","print(dt.astimezone(IST))"]},{"cell_type":"code","execution_count":null,"id":"f434eac5-aff9-4642-80ac-f2ea255cc20c","metadata":{},"outputs":[],"source":["# Time zone database\n","\n","# Imports\n","from datetime import datetime\n","from dateutil import tz\n","\n","# Eastern time\n","et = tz.gettz('America/New_York')\n","\n","# Ending Daylight Saving Time\n","eastern = tz.gettz('US/Eastern')\n","# 2017-11-05 01:00:00\n","first_1am = datetime(2017, 11, 5, 1, 0, 0, tzinf=eastern)\n","tz.datetime_ambiguous(first_1am)\n","# 2017-11-05 01:00:00 again\n","second_1am = datetime(2017, 11, 5, 1, 0, 0, tzinfo=eastern)\n","second_1am = tz.enfold(second_1am)"]},{"cell_type":"markdown","id":"706f41ef-0857-426e-b796-b8c6b8a5b9a7","metadata":{},"source":["### Reading date and time data in Pandas"]},{"cell_type":"code","execution_count":null,"id":"52bec693-e016-4bcf-8c9d-3886bf6fe9cf","metadata":{},"outputs":[],"source":["# Loading datetimes with parse_dates\n","\n","# Import W20529's rides in Q4 2017\n","rides = pd.read_csv('capital-onebike.csv', \n","                    parse_dates = ['Start date', 'End date'])\n","# Or: \n","rides['Start date'] = pd.to_datetime(rides['Start date'], \n","                                     format = \"%Y-%m-%d %H:%M:%S\")"]},{"cell_type":"code","execution_count":null,"id":"1655ddb1-2663-470a-9f28-ad99fd3b4618","metadata":{},"outputs":[],"source":["# Timezone aware arithmetic\n","\n","# Create a duration column\n","rides['Duration'] = rides['End date'] - rides['Start date']\n","\n","# Corverting our 'Duration' column to seconds\n","rides['Duration']\\\n","    .dt.total_seconds()\\\n","    .head(5)"]},{"cell_type":"code","execution_count":null,"id":"7a31ae37-2785-4bf4-b178-26e8e9e3ea78","metadata":{},"outputs":[],"source":["# Sumarizing data in Pandas\n","\n","# Percent of time out of the dock\n","rides['Duration'].sum() / timedelta(days=91)\n","\n","# Percent of rides by member\n","rides['Member type'].value_counts() / len(rides)\n","\n","# Add duration (in seconds) column\n","rides['Duration seconds'] = rides['Duration'].dt.total_seconds()\n","# Average duration per member type\n","rides.groupby('Member type')['Duration seconds'].mean()\n","\n","# Average duration by month\n","rides.resample('M', on = 'Start date')['Duration seconds'].mean()\n","\n","# Size per group\n","rides.groupby('Member type').size()\n","\n","# First ride per group\n","rides.groupby('Member type').first()"]},{"cell_type":"code","execution_count":null,"id":"cce92b71-932f-4249-903f-39c73b5ed21e","metadata":{},"outputs":[],"source":["# Try to set a timezone\n","rides['Start date'] = rides['Start date']\\\n","    .dt.tz_localize('America/New_York', ambiguous='NaT')\n","# without 'ambigous' we'll down in to an error\n","\n","# Shift the indexes forward one, padding with NaT\n","rides['End date'].shift(1).head(3)"]},{"cell_type":"markdown","id":"83c21d8a-41cc-405e-a95a-77edb92b7b47","metadata":{},"source":["# Writing functions in Python"]},{"cell_type":"code","execution_count":null,"id":"c25d679a-e1c0-4b98-ad1e-c68bb8eee4f3","metadata":{},"outputs":[],"source":["# Docstrings\n","def the_answer():\n","    '''Return the answer ot life,\n","    the universe, and everything\n","    \n","    Returns:\n","        int\n","    '''\n","\n","# If I want to access the docstring of a function\n","import inspect\n","print(inspect.getdoc(the_answer))"]},{"cell_type":"code","execution_count":null,"id":"8b6f310f-1a3c-4e85-af4c-0b48b58bff39","metadata":{},"outputs":[],"source":["# Using context managers\n","with <context-manager>(<args>) as <variable-name>:\n","    # Run your code here\n","    # This code is running \"inside the context\"\n","\n","# This code runs after the context is removed"]},{"cell_type":"markdown","id":"139f3b00-e6e0-481f-84c2-ef5549b28f46","metadata":{},"source":["###  The open() function does three things\n","- Sets up a context by opening a file\n","- Lets you run any code you want on that file\n","- Removes the context by closing the file"]},{"cell_type":"code","execution_count":null,"id":"e1e041e9-a566-4a2d-96f1-7199fd47485f","metadata":{},"outputs":[],"source":["# A real-world example\n","with open('my_file.txt') as my_file:  \n","    text = my_file.read()  \n","    length = len(text)\n","\n","print('The file is {} characters long'.format(length))"]},{"cell_type":"markdown","id":"617a7d72-2862-4b7b-a69e-5b0394b158f9","metadata":{},"source":["### Two ways to define a context managers\n","- Class-based\n","- Function-based"]},{"cell_type":"code","execution_count":null,"id":"fc503367-4a9a-489b-95cb-e5ac11267ea1","metadata":{},"outputs":[],"source":["# Writing context managers\n","def mt_context():\n","    # Add any set up code you need\n","    yield\n","    # Add any teardown code you need"]},{"cell_type":"markdown","id":"f044f9e0-2e37-477f-a1ea-f4025ac7e916","metadata":{},"source":["### How to create a context manager\n","1. Define a function\n","2. (optional) Add any set up code your context needs\n","3. Use the \"yield keyword\"\n","4. (optional) Add any teardown code your context needs\n","5. Add the 'contextlib.contextmanager' decorator"]},{"cell_type":"code","execution_count":null,"id":"e92efe20-2c52-4194-8a24-2505aa9c1c51","metadata":{},"outputs":[],"source":["# The \"yield\" keyword\n","@contextlib.contextmanagerdefmy_context():  \n","    print('hello')\n","    yield 42\n","    # It means that you are going to return a value,\n","    # but you expect to finish the rest of the function at some point the future.\n","    print('goodbye')"]},{"cell_type":"code","execution_count":null,"id":"ef652618-0dd0-48d0-8998-9bd42d470538","metadata":{},"outputs":[],"source":["with my_context() as foo:  \n","    print('foo is {}'.format(foo))"]},{"cell_type":"code","execution_count":null,"id":"0839b138-1fc2-47c3-9bd6-60b37f3e231b","metadata":{},"outputs":[],"source":["# Setup and teardown\n","@contextlib.contextmanager\n","def database(url):\n","    # set up database connection  \n","    db = postgres.connect(url)\n","    \n","    yield db\n","    \n","    # tear down database connection  \n","    db.disconnect()"]},{"cell_type":"code","execution_count":null,"id":"56b60641-e011-4374-b196-e0c8ae22fb94","metadata":{},"outputs":[],"source":["# Yielding a value or None\n","url = 'http://datacamp.com/data'\n","with database(url) as my_db:  \n","    course_list = my_db.execute('SELECT * FROM courses')"]},{"cell_type":"code","execution_count":null,"id":"ea300e9b-cdd0-48b7-9bb2-e7eec5d9ff7f","metadata":{},"outputs":[],"source":["'''\n","Changes the current working directory\n","to a specific path and then changes it \n","back after the context block is done\n","'''\n","@contextlib.contextmanager\n","de fin_dir(path):\n","    # save current working directory\n","    old_dir = os.getcwd()\n","    \n","    # switch to new working directory\n","    os.chdir(path)\n","    \n","    yield\n","    # change back to previous\n","    # working directory\n","    os.chdir(old_dir)"]},{"cell_type":"code","execution_count":null,"id":"9e8792bd-fdb7-46a5-af19-06b65a0cf194","metadata":{},"outputs":[],"source":["with in_dir('/data/project_1/'):\n","    project_files = os.listdir()"]},{"cell_type":"markdown","id":"1ebbd90e-42c2-4ca1-8029-4a4197591351","metadata":{},"source":["## Advanced topics"]},{"cell_type":"code","execution_count":null,"id":"db326516-9b96-4856-bb97-981fa4bdabff","metadata":{},"outputs":[],"source":["# Nested contexts\n","def copy(src, dst):\n","    \"\"\"\n","    Copy the contents of one file to another.\n","    \n","    Args:\n","        src (str): File name of the file to be copied.\n","        dst (str): Where to write the new file.\n","    \"\"\"\n","# Open both files\n","with open(src) as f_src:\n","    with open(dst, 'w') as f_dst:\n","        # Read and write each line, one at a time\n","        for line in f_src:\n","            f_dst.write(line)"]},{"cell_type":"code","execution_count":null,"id":"bf0f8665-0b19-47ce-b5fd-f7775e9d77f9","metadata":{},"outputs":[],"source":["# Handling errors\n","def get_printer(ip):\n","    p = connect_to_printer(ip)\n","    \n","    try:\n","        yield\n","    \n","    # This MUST be called or no one else will\n","    # be able to connect to the printer\n","    finally:\n","        p.disconnect()\n","    print('disconnected from printer')\n","    \n","doc = {'text': 'This is my text.'}"]},{"cell_type":"markdown","id":"4c1b6863-8647-4bd3-b1c3-dabd335f4068","metadata":{},"source":["### Context manager patterns\n","- Open / Close\n","- Lock / Release\n","- Change / Reset\n","- Enter / Exit\n","- Start / Stop\n","- Setup / Teardown\n","- Connect / Disconnect"]},{"cell_type":"markdown","id":"2ca7ab03-47a8-4ca9-84f9-e9c74b1f41f1","metadata":{},"source":["## Decorators"]},{"cell_type":"code","execution_count":null,"id":"3798e2fb-eefd-42ab-b405-2079cc456ec9","metadata":{},"outputs":[],"source":["# Functions as objects\n","\n","# Everything here is an object\n","defx():\n","    pass\n","x = [1, 2, 3]\n","x = {'foo': 42}\n","x = pandas.DataFrame()\n","x = 'This is a sentence.'\n","x = 3\n","x = 71.2\n","import x"]},{"cell_type":"code","execution_count":null,"id":"a0910d96-05dd-46a7-9e29-674202671d51","metadata":{},"outputs":[],"source":["# Function as variables\n","def my_function():  \n","    print('Hello')\n","x = my_function\n","print(type(x))\n","\n","PrintyMcPrintface = print\n","PrintyMcPrintface('Python is awesome!')Python is awesome!"]},{"cell_type":"code","execution_count":null,"id":"e86f9383-a2da-4633-a663-bd09512db5ab","metadata":{},"outputs":[],"source":["# Lists and dictionaries of functions\n","list_of_functions = [my_function, open, print]\n","list_of_functions[2]('I am printing with an element of a list!')\n","\n","dict_of_functions = {\n","    'func1': my_function,\n","    'func2': open,\n","    'func3': print\n","}\n","dict_of_functions['func3']('I am printing with a value of a dict!')"]},{"cell_type":"code","execution_count":null,"id":"861a8f88-a5c7-497a-843d-bbd82b2faa2b","metadata":{},"outputs":[],"source":["# Functions as arguments\n","def has_docstring(func):\n","    \"\"\"Check to see if the function\n","    `func` has a docstring.\n","    \n","    Args:\n","        func (callable): A function.\n","    Returns:\n","        bool\n","    \"\"\"\n","return func.__doc__ is not None\n","\n","# Examples of functions as arguments\n","def no():\n","    return 42\n","\n","def yes():\n","    \"\"\"Return the value 42  \"\"\"\n","    return42\n","    \n","print(has_docstring(no))\n","print(has_docstring(yes))"]},{"cell_type":"markdown","id":"7898ed93-aed9-4aab-a131-3bd0545a0087","metadata":{},"source":["## Closures"]},{"cell_type":"code","execution_count":null,"id":"ebd2f92d-4d01-401f-8371-3fa2c441bcee","metadata":{},"outputs":[],"source":["# Attaching non local variables to nested functions\n","def foo():\n","    a = 5\n","    def bar():\n","        print(a)\n","    return bar\n","\n","func = foo()\n","\n","func()"]},{"cell_type":"code","execution_count":null,"id":"cfd2f4ca-f54a-4d54-93fc-9df3c7ec8eea","metadata":{},"outputs":[],"source":["# Closures!\n","print(type(func.__closure__))\n","print(len(func.__closure__))\n","print(func.__closure__[0].cell_contents)"]},{"cell_type":"markdown","id":"a7ecdaf8-94be-4296-a819-18bac15b0a03","metadata":{},"source":["**Nested function:** A function defined inside another function"]},{"cell_type":"code","execution_count":null,"id":"7367af38-b923-4210-b514-06cd5f6fbda9","metadata":{},"outputs":[],"source":["# Outer function\n","def parent():\n","    # nested function\n","    def child():\n","        pass\n","    return child"]},{"cell_type":"markdown","id":"4198b69b-d19e-4570-b83b-a464967a37e8","metadata":{},"source":["**Nonlocal variables**: Variables defined in the parent function that are used by the child function"]},{"cell_type":"code","execution_count":null,"id":"f0641266-6c79-4087-9680-1a15d4b1a825","metadata":{},"outputs":[],"source":["def parent(arg_1, arg_2):\n","    # From child()'s point of view,\n","    # `value` and `my_dict` are nonlocal variables,\n","    # as are `arg_1` and `arg_2`.\n","    value = 22\n","    my_dict = {'chocolate': 'yummy'}\n","    \n","    def child():\n","        print(2 * value)\n","        print(my_dict['chocolate'])\n","        print(arg_1 + arg_2)\n","        \n","    return child"]},{"cell_type":"markdown","id":"65c6c95c-528c-488e-9502-848147c2f915","metadata":{},"source":["**Closure**: Nonlocal variables attached to a returned function"]},{"cell_type":"code","execution_count":null,"id":"f6ae31b2-3e04-4372-bfa8-acad30cd1796","metadata":{},"outputs":[],"source":["def parent(arg_1, arg_2):\n","    value = 22\n","    my_dict = {'chocolate': 'yummy'}\n","\n","    def child():\n","        print(2 * value)\n","        print(my_dict['chocolate'])\n","        print(arg_1 + arg_2)\n","    \n","    return child\n","\n","new_function = parent(3, 4)\n","\n","print([cell.cell_contents for cell in new_function.__closure__])"]},{"cell_type":"code","execution_count":null,"id":"f1927888-3899-4c65-bac0-22fc3110c7c4","metadata":{},"outputs":[],"source":["# Finally, we'll talk about Decorators\n","\n","# What does a decorator look like?\n","@double_args\n","def multiply(a, b):\n","    return a * b\n","multiply(1, 5)"]},{"cell_type":"code","execution_count":null,"id":"0c8bd2e4-72d0-49a5-b645-15945e74ebd2","metadata":{},"outputs":[],"source":["# The double_args decorator (an example about how decorators works)\n","def multiply(a, b):\n","    return a * b\n","def double_args(func):\n","    def wrapper(a, b):\n","        return func(a * 2, b * 2)\n","    return wrapper \n","# multiply = double_args(multiply) --> we can combine this, but it's the same result\n","multiply(1, 5)"]},{"cell_type":"markdown","id":"77b14836-043c-432a-8141-18bfbf01cdba","metadata":{},"source":["## More on decorators"]},{"cell_type":"code","execution_count":null,"id":"a68bf198-b313-410f-a4b2-36678e8eec95","metadata":{},"outputs":[],"source":["# Time a function\n","import time\n","\n","def timer(func):\n","    \"\"\"A decorator that prints how long a function took to run.\"\"\"\n","    # Define the wrapper function to return.\n","    def wrapper(*args, **kwargs):\n","        # When wrapper() is called, get the current time.\n","        t_start = time.time()\n","        # Call the decorated function and store the result.\n","        result = func(*args, **kwargs)\n","        # Get the total time it took to run, and print it.\n","        t_total = time.time() - t_start\n","        print('{} took {}s'.format(func.__name__, t_total))\n","        return result\n","    return wrapper"]},{"cell_type":"code","execution_count":null,"id":"be7731a5-8613-4c82-a93d-e69196016e80","metadata":{},"outputs":[],"source":["# Using timer()\n","@timer\n","def sleep_n_seconds(n):\n","    time.sleep(n)\n","    \n","sleep_n_seconds(10)"]},{"cell_type":"code","execution_count":null,"id":"cf84dd70-bc29-4348-a042-c3c49f8436c6","metadata":{},"outputs":[],"source":["# Storing the results of a function\n","def memoize(func):\n","    \"\"\"Store the results of the decorated function for fast lookup\"\"\"\n","    # Store results in a dict that maps arguments to results\n","    cache = {}\n","    # Define the wrapper function to return.\n","    def wrapper(*args, **kwargs):\n","        # If these arguments haven't been seen before,\n","        if (args, kwargs) notin cache:\n","            # Call func() and store the result.\n","            cache[(args, kwargs)] = func(*args, **kwargs)\n","            return cache[(args, kwargs)]\n","        return wrapper"]},{"cell_type":"code","execution_count":null,"id":"83b6828d-c13c-4863-897d-955da5ac2ce4","metadata":{},"outputs":[],"source":["# Using memoize()\n","@memoize\n","def slow_function(a, b):\n","    print('Sleeping...')\n","    time.sleep(5)\n","    return a + b\n","\n","slow_function(3, 4)"]},{"cell_type":"markdown","id":"7f797a4c-e053-4138-987f-237b13f58135","metadata":{},"source":["### When to use decorators\n","- Add commom behavior to multiple functions"]},{"cell_type":"code","execution_count":null,"id":"4d13eb21-48fa-4947-b58a-c82ce5d31ba4","metadata":{},"outputs":[],"source":["# Decorators and metadata\n","from functools import wraps\n","def timer(func):\n","    \"\"\"A decorator that prints how long a function took to run.\"\"\"\n","    @wraps(func)\n","    def wrapper(*args, **kwargs):\n","        t_start = time.time()\n","        result = func(*args, **kwargs)\n","        t_total = time.time() - t_start\n","        print('{} took {}s'.format(func.__name__, t_total))\n","        return result \n","    return wrapper"]},{"cell_type":"markdown","id":"1eb52e5c-a633-4efd-a73f-ea817ee7f12d","metadata":{},"source":["## Decorators that take arguments"]},{"cell_type":"code","execution_count":null,"id":"34c6b304-6576-4d7b-a2bb-5e77056f3509","metadata":{},"outputs":[],"source":["def run_n_times(n):\n","    \"\"\"Define and return a decorator\"\"\"\n","    def decorator(func):\n","        def wrapper(*args, **kwargs):\n","            for i in range(n):\n","                func(*args, **kwargs)\n","                return wrapper \n","            return decorator\n","        run_three_times = run_n_times(3)\n","@run_three_times \n","def print_sum(a, b):\n","    print(a + b)\n","@run_n_times(3)\n","def print_sum(a, b):\n","    print(a + b)"]},{"cell_type":"markdown","id":"303da81f-8e98-41b3-89d0-471287f0caa0","metadata":{},"source":["### Timeout - background info"]},{"cell_type":"code","execution_count":null,"id":"c3097712-5924-4b6c-9b45-3692f22b817e","metadata":{},"outputs":[],"source":["import signal\n","def raise_timeout(*args, **kwargs):\n","    raise TimeoutError()\n","# When an \"alarm\" signal goes off, call raise_timeout()\n","signal.signal(signalnum=signal.SIGALRM, handler=raise_timeout)\n","# Set off an alarm in 5 seconds\n","signal.alarm(5)\n","# Cancel the alarm\n","signal.alarm(0)"]},{"cell_type":"markdown","id":"2ea2bc7c-01d2-4156-aa38-7b51f3a08540","metadata":{},"source":["### Timeout itself"]},{"cell_type":"code","execution_count":null,"id":"20c2a33a-0cf1-4b6a-a1ae-b8824638c06c","metadata":{},"outputs":[],"source":["def timeout(n_seconds):\n","    def decorator(func):\n","        @wraps(func)\n","        def wrapper(*args, **kwargs):\n","            # Set an alarm for n seconds\n","            signal.alarm(n_seconds)\n","            try:\n","                # Call the decorated func\n","                return func(*args, **kwargs)\n","            finally:\n","                # Cancel alarm\n","                signal.alarm(0)\n","        return wrapper\n","    return decorator"]},{"cell_type":"markdown","id":"896ecf0b-b87e-4dfe-b8b9-92805bec3dc4","metadata":{},"source":["# Introduction to Regression with statsmodels in Python"]},{"cell_type":"markdown","id":"bc887af6-c490-495f-a92a-7bc47725a428","metadata":{},"source":["## What is regression?\n","1. Statistical models to explore the relationship a response variable and some explanatory variables.\n","2. Given values of explanatory variables, you can predict the values of the response variable.\n","\n","### Jargon\n","**Response variable (a.k.a. dependent variable):**\n","- The variable that you want to predict.\n","**Explanatory variables (a.k.a. independent variables):**\n","- The variables that explain how the response variable will change."]},{"cell_type":"markdown","id":"6fe4c756-c9f0-40a9-bab5-7be4152895b8","metadata":{},"source":["## Linear Regression and Logistic Regression\n","**Linear regression:**\n","- The response variable is numeric.\n","**Logistic regression:**\n","- The response variable is logical.\n","**Simple linear/logistic regression:**\n","- There is only one explanatory variable.\n","\n","### Straight lines are defined by two things\n","**Intercept:**\n","- The value at the point when x is zero.\n","**Slope:**\n","-The amount the y value increases if you increase x by one.\n","**Equation:**\n","- y = intercept + slope * x."]},{"cell_type":"code","execution_count":null,"id":"b4105538-5ed9-492b-83c6-5173a41c0611","metadata":{},"outputs":[],"source":["# Making predictions\n","from statsmodels.formula.api import ols # ordinary least squares\n","                                        # it's a type of regression\n","# Running the model\n","mdl_mass_vs_length = ols(\"mass_g ~ length_cm\", data=bream).fit()\n","print(mdl_mass_vs_length.params)\n","\n","# Predicting inside a DataFrame\n","explanatory_data = pd.DataFrame(\n","    {\"length_cm\": np.arange(20, 41)}\n",")\n","prediction_data = explanatory_data.assign(    mass_g=mdl_mass_vs_length.predict(explanatory_data)\n",")\n","print(prediction_data)\n","\n","# Extrapolating means (making predictions outside the range of observed data)\n","little_bream = pd.DataFrame({\"length_cm\": [10]})\n","\n","pred_little_bream = little_bream.assign(    mass_g=mdl_mass_vs_length.predict(little_bream))\n","\n","print(pred_little_bream)"]},{"cell_type":"code","execution_count":null,"id":"150b4128-2009-4938-9a50-d80cb48c2b5d","metadata":{},"outputs":[],"source":["# Regression to the mean\n","'''\n","Response value = fitted value + residual\n","or\n","               = 'the stuff you explained' + 'the stuff you couldn't explain\n","obs: residual exist due to problems in the model and fundamental randomness\n","'''\n","# Adding a regression line in scatter plot\n","fig = plt.figure()\n","\n","sns.regplot(x=\"father_height_cm\",                \n","                y=\"son_height_cm\",                \n","                data=father_son,\n","                ci = None,\n","                line_kws={'color':'black'})\n","plt.axline(xy1=(150, 150),\n","           slope=1,\n","           linewidth=2,\n","           color=\"green\")\n","plt.axis(\"equal\")\n","\n","plt.show()"]},{"cell_type":"markdown","id":"89a01c2a-9b9a-469b-8558-c3634c9e6233","metadata":{},"source":["## Coefficient of determination (Quantifying model fit) \n","- Sometimes called \"r-squared\" or \"R-squared\".\n","**The proportion of the variance in the response variable that is predictable from the explanatory variable.**\n","- 1 means a perfect fit \n","- 0 means the worst possible fit."]},{"cell_type":"code","execution_count":null,"id":"9e86bd93-4a29-4442-b00b-62fdb85bb231","metadata":{},"outputs":[],"source":["# Shows several performance metrics in its output\n","mdl_bream = ols(\"mass_g ~ length_cm\", data=bream).fit() #apply ML model\n","\n","print(mdl_bream.summary()) # see the metrics of its model\n","# or you can use 'print(mdl_bream.rsquared)' to be more specific"]},{"cell_type":"markdown","id":"6ece11ed-84e9-4286-8db5-7e074ba71d2c","metadata":{},"source":["### Residual Stardanrd Error (RSE)\n","1. A \"typical\" difference between a prediction and an observed response.\n","2. It has the same unit as the response variable.\n","3. MSE = RSE²."]},{"cell_type":"code","execution_count":null,"id":"a7c5ff02-41bf-48a1-ba06-07944b22b9b7","metadata":{},"outputs":[],"source":["# Calculate the mse\n","mse = mdl_bream.mse_resid\n","print('mse: ', mse)\n","\n","# Calculate the RSE\n","rse = np.sqrt(mse)\n","print('rse: ', rse)\n","\n","# Calculating the RSE from \"scratch\"\n","residuals_sq = mdl_bream.resid ** 2\n","\n","resid_sum_of_sq = sum(residuals_sq)\n","\n","deg_freedom = len(bream.index) - 2\n","'''\n","Degrees of freedom equals the number of observations\n","minus the number of model coefficients.\n","'''\n","rse = np.sqrt(resid_sum_of_sq / deg_freedom)\n","\n","print('rse: ', rse)\n","\n","# What's the difference between RSE and RMSE\n","# In RMSE we don't calculate degrees of freedon, it's just the number of observations\n","residuals_sq = mdl_bream.resid ** 2\n","\n","resid_sum_of_sq = sum(residuals_sq)\n","\n","n_obs = len(bream.index)\n","\n","rmse = np.sqrt(resid_sum_of_sq / n_obs)\n","\n","print('rmse: ', rmse)"]},{"cell_type":"markdown","id":"74281c7f-dee7-4ee4-8ea0-002054eee93c","metadata":{},"source":["## Visualizing model fit"]},{"cell_type":"code","execution_count":null,"id":"1e2ab858-7f92-40b6-927f-dc13bd3675a6","metadata":{},"outputs":[],"source":["# Residual vs. fitted values\n","'''\n","Here you can see diagnostic plots of residuals versus \n","fitted values for two models on advertising conversion.\n","'''\n","sns.residplot(x=\"length_cm\", y=\"mass_g\", data=bream, lowess=True)\n","plt.xlabel(\"Fitted values\")\n","plt.ylabel(\"Residuals\")\n","\n","# GGplot\n","from statsmodels.api import qqplot\n","qqplot(data=mdl_bream.resid, fit=True, line=\"45\")\n","\n","# Scale-location plot\n","'''\n","Here are normal scale-location plots of the previous two models.\n","That is, they show the size of residuals versus fitted values.\n","'''\n","model_norm_residuals_bream = mdl_bream.get_influence().resid_studentized_internal\n","model_norm_residuals_abs_sqrt_bream = np.sqrt(np.abs(model_norm_residuals_bream))\n","\n","sns.regplot(x=mdl_bream.fittedvalues, y=model_norm_residuals_abs_sqrt_bream,\n","            ci=None, lowess=True)\n","\n","plt.xlabel(\"Fitted values\")\n","plt.ylabel(\"Sqrt of abs val of stdized residuals\")"]},{"cell_type":"markdown","id":"7631dee7-5fd5-4b40-8438-a18813338eb0","metadata":{},"source":["### Outliers, leverage and influence"]},{"cell_type":"code","execution_count":1,"id":"2f2087f0-902a-4289-bf2b-eada3d80fc13","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1687715629521,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"outputs":[],"source":["# .get_influence()and.summary_frame()\n","mdl_roach = ols(\"mass_g ~ length_cm\", data=roach).fit()\n","summary_roach = mdl_roach.get_influence().summary_frame()\n","roach[\"leverage\"] = summary_roach[\"hat_diag\"]"]},{"cell_type":"markdown","id":"d5e7c20c-e071-4639-9f00-096c035018a9","metadata":{},"source":["## Logistic Regression"]},{"cell_type":"code","execution_count":null,"id":"a21d1504-3ff4-4770-b7b6-136e1e7886ed","metadata":{},"outputs":[],"source":["from statsmodels.formula.api import logit\n","mdl_churn_vs_recency_logit = logit(\"has_churned ~ time_since_last_purchase\",                                   data=churn).fit()\n","print(mdl_churn_vs_recency_logit.params)\n","\n","sns.regplot(x=\"time_since_last_purchase\", \n","            y=\"has_churned\", data=churn, \n","            ci=None, logistic=True)\n","plt.axline(xy1=(0,intercept), slope=slope, color=\"black\")\n","\n","plt.show()"]},{"cell_type":"markdown","id":"d9ceed1d-25d8-4b25-9b68-3eea0ad379ce","metadata":{},"source":["### Predictions and odds ratios\n","**Odds ratio**\n","- Odds ratio is the probability of something happening divided by the probability that it doesn't"]},{"cell_type":"code","execution_count":null,"id":"77861848-c72b-40a0-bf75-7a2524f376bd","metadata":{},"outputs":[],"source":["## Visualizing odds ratio\n","prediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] /                                 (1 - prediction_data[\"has_churned\"])\n","\n","sns.lineplot(x=\"time_since_last_purchase\", y=\"odds_ratio\", data=prediction_data)\n","\n","plt.axhline(y=1, linestyle=\"dotted\")\n","plt.yscale(\"log\")\n","\n","plt.show()"]},{"cell_type":"markdown","id":"008c0d54-3282-4fc7-b696-8a3361654f0b","metadata":{},"source":["## Quantifying logistic regression fit"]},{"cell_type":"code","execution_count":null,"id":"5f0453f4-105e-44d0-a856-dd17c5fb69af","metadata":{},"outputs":[],"source":["# Confusion matrix ('Implementation')\n","actual_response = churn[\"has_churned\"]\n","\n","predicted_response = np.round(mdl_recency.predict())\n","\n","outcomes = pd.DataFrame({\"actual_response\": actual_response,\"predicted_response\": predicted_response})\n","\n","print(outcomes.value_counts(sort=False))\n","\n","# Visualizing the confusion matrix\n","conf_matrix = mdl_recency.pred_table()\n","\n","from statsmodels.graphics.mosaicplot import mosaic\n","\n","mosaic(conf_matrix)"]},{"cell_type":"markdown","id":"3d671120-259a-451c-9fcc-7d618b68aacb","metadata":{},"source":["# Sampling in Python"]},{"cell_type":"markdown","id":"e773a80b-544e-45f6-aa16-f89b69302751","metadata":{},"source":["## Population vs. sample\n","**The population is the complete datase**\n","- Doesn't have to refer to people\n","- Typically, don't know what the whole population is\n","\n","\n","**The sample is the subset of data you calculate on**\n","\n","\n","## Population parameters & point estimates\n","1. A population parameter is a calculation made on the population dataset.\n","2. A point estimate or sample statistic is a calculation made on the sample dataset."]},{"cell_type":"code","execution_count":null,"id":"81b08a71-a526-4b36-896f-e60187b5009c","metadata":{},"outputs":[],"source":["# Population parameter\n","import numpy as np\n","np.mean(pts_vs_flavor_pop['total_cup_points'])\n","\n","# Point estimate\n","cup_points_samp = coffee_ratings['total_cup_points'].sample(n=10)\n","np.mean(cup_points_samp)"]},{"cell_type":"code","execution_count":null,"id":"bcb42aed-ec38-44a0-8d4f-e505bd89e1e8","metadata":{},"outputs":[],"source":["# Visualizing selection bias\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Population distribution\n","coffee_ratings[\"total_cup_points\"].hist(bins=np.arange(59, 93, 2))\n","plt.show()\n","\n","# Sample distribution\n","coffee_ratings_first10[\"total_cup_points\"].hist(bins=np.arange(59, 93, 2))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"05c484b7-548b-4015-83c4-641ab734048a","metadata":{},"outputs":[],"source":["# Pseudo-random number generation \n","randoms = np.random.beta(a=2, b=2, size=5000)\n","plt.hist(randoms, bins=np.arange(0, 1, 0.05))\n","plt.show()\n","\n","# Random number seeds\n","np.random.seed(20000229)\n","print(np.random.normal(loc=2, scale=1.5, size=2))\n","print(np.random.normal(loc=2, scale=1.5, size=2))\n","print(np.random.normal(loc=2, scale=1.5, size=2))"]},{"cell_type":"markdown","id":"4c5bc9ad-9f47-4a5e-9984-618f5b4214ea","metadata":{},"source":["## Simple random and systematic sampling"]},{"cell_type":"code","execution_count":null,"id":"5d1ad824-5016-4297-84f5-4e406b3e7e55","metadata":{},"outputs":[],"source":["# Simple random sampling \n","coffee_ratings.sample(n=5, random_state=19000113)\n","\n","# Systematic sampling \n","sample_size = 5\n","pop_size = len(coffee_ratings)\n","\n","# Defining the interval\n","interval = pop_size // sample_size\n","\n","# Selecting the rows\n","coffee_ratings.iloc[::interval]\n","'''\n","OBS: Systematic sampling is only safe if we \n","don't see a pattern in this scatter plot\n","'''\n","# Making systematic sampling safe\n","shuffled = coffee_ratings.sample(frac=1)\n","shuffled = shuffled.reset_index(drop=True).reset_index()\n","shuffled.plot(x=\"index\", y=\"aftertaste\", kind=\"scatter\")\n","'''\n","OBS: Shuffling rows + systematic sampling\n","is the same as simple random sampling\n","'''"]},{"cell_type":"markdown","id":"2e7df3e1-40f5-4e29-bbbf-ee4f0210e607","metadata":{},"source":["## Stratified and weighted random sampling"]},{"cell_type":"code","execution_count":null,"id":"b63e1dd9-b840-40c5-922b-4dcd75794667","metadata":{},"outputs":[],"source":["# Counts of a simple random sample\n","coffee_ratings_samp = coffee_ratings_top.sample(frac=0.1, random_state=2021)\n","coffee_ratings_samp['country_of_origin'].value_counts(normalize=True)\n","\n","# Proportional stratified sampling\n","coffee_ratings_strat = coffee_ratings_top.groupby(\"country_of_origin\").sample(frac=0.1, random_state=2021)\n","coffee_ratings_strat['country_of_origin'].value_counts(normalize=True)\n","\n","# Equal counts stratified sampling\n","coffee_ratings_eq = coffee_ratings_top.groupby(\"country_of_origin\").sample(n=15, random_state=2021)\n","coffee_ratings_eq['country_of_origin'].value_counts(normalize=True)\n","\n","# Weighted random sampling\n","# Specify weights to adjust the relative probability of a row being sampled\n","import numpy as np\n","\n","coffee_ratings_weight = coffee_ratings_top\n","condition = coffee_ratings_weight['country_of_origin'] == \"Taiwan\"\n","coffee_ratings_weight['weight'] = np.where(condition, 2, 1)\n","coffee_ratings_weight = coffee_ratings_weight.sample(frac=0.1, weights=\"weight\")"]},{"cell_type":"markdown","id":"d7fbfb2c-f035-48c6-bab5-718d12c64399","metadata":{},"source":["## Clustering sampling\n","\n","### Stratified sampling vs. cluster sampling\n","**Stratified sampling:**\n","- Split the population into subgroups.\n","- Use simple random sampling on every subgroup.\n","**Cluster sampling:**\n","- Use simple random sampling to pick some subgroups.\n","- Use simple random sampling on only those subgroups."]},{"cell_type":"code","execution_count":null,"id":"3b736a9a-e9e0-4c3e-94c6-2d40912bd7d3","metadata":{},"outputs":[],"source":["# Stage 1: sampling for subgroups\n","import random\n","varieties_samp = random.sample(varieties_pop, k=3)\n","\n","# Stage 2: sampling each group\n","variety_condition = coffee_ratings['variety'].isin(varieties_samp)\n","coffee_ratings_cluster = coffee_ratings[variety_condition]\n","\n","coffee_ratings_cluster['variety'] = coffee_ratings_cluster['variety'].cat.remove_unused_categories()\n","\n","coffee_ratings_cluster.groupby(\"variety\").sample(n=5, random_state=2021)"]},{"cell_type":"markdown","id":"b479e99c-6ac9-406f-9bce-bacea9ce8bd5","metadata":{},"source":["### Multistage sampling\n","- Cluster sampling is a type of multistage sampling\n","- Can have > 2 stages\n","- E.g., countrywide surveys may sample states, counties, cities, and neighborhoods\n"]},{"cell_type":"markdown","id":"312765e2-6b14-480a-904b-25cd86239f92","metadata":{},"source":["## Sampling Distributions"]},{"cell_type":"markdown","id":"cdd2edde-2f3a-4697-a136-deca052ae7bf","metadata":{},"source":["### Relative error of point estimates\n","\n","Properties:\n","- Really noise, particularly for small samples\n","- Amplitude is initially steep, then flattens\n","- Relative error decreases to zero (when thesample size = population)"]},{"cell_type":"code","execution_count":null,"id":"63978b10-450e-4324-a6b6-5822cae8df4c","metadata":{},"outputs":[],"source":["# Population parameter:\n","population_mean = coffee_ratings['total_cup_points'].mean()\n","\n","# Point estimate\n","sample_mean = coffee_ratings.sample(n=sample_size)['total_cup_points'].mean()\n","\n","# Relative error as a percentage:\n","rel_error_pct = 100 * abs(population_mean - sample_mean) / population_mean"]},{"cell_type":"markdown","id":"3dd59598-743b-4310-8d13-1f4d1a49e48c","metadata":{},"source":["### Creating a sampling distribution"]},{"cell_type":"code","execution_count":null,"id":"85b36344-5423-4cf1-a08b-6bd0da7bd679","metadata":{},"outputs":[],"source":["mean_cup_points_1000 = []\n","\n","for i in range(1000):\n","    mean_cup_points_1000.append(\n","        coffee_ratings.sample(n=30)['total_cup_points'].mean()\n","    )\n","\n","print(mean_cup_points_1000)"]},{"cell_type":"markdown","id":"42bc6efe-8687-46c8-af7f-9a8f511542a0","metadata":{},"source":["### Aproximate sampling distributions"]},{"cell_type":"code","execution_count":null,"id":"4ecc5235-df6a-4154-8a22-4124f834a9ac","metadata":{},"outputs":[],"source":["# Create a DataFrame an Calculate the Mean\n","dice = expand_grid(  {'die1': [1, 2, 3, 4, 5, 6],\n","                      'die2': [1, 2, 3, 4, 5, 6],\n","                      'die3': [1, 2, 3, 4, 5, 6],\n","                      'die4': [1, 2, 3, 4, 5, 6]\n","                     }\n","                  )\n","dice['mean_roll'] = (dice['die1'] +\n","                     dice['die2'] +\n","                     dice['die3'] +\n","                     dice['die4']) / 4\n","print(dice)"]},{"cell_type":"code","execution_count":null,"id":"40035ebb-6e5c-4420-9665-af5b4c2edb3d","metadata":{},"outputs":[],"source":["# Exact sampling distribution\n","dice['mean_roll'] = dice['mean_roll'].astype('category')\n","dice['mean_roll'].value_counts(sort=False).plot(kind=\"bar\")"]},{"cell_type":"code","execution_count":null,"id":"e5059de8-83ed-49e5-8c57-28cd691c0731","metadata":{},"outputs":[],"source":["# The number of outcomes increases fast\n","n_dice = list(range(1, 101))\n","n_outcomes = []\n","for n in n_dice:\n","    n_outcomes.append(6**n)\n","    outcomes = pd.DataFrame(\n","        {\"n_dice\": n_dice,\n","         \"n_outcomes\": n_outcomes})\n","\n","# Plot the results\n","outcomes.plot(x=\"n_dice\",\n","              y=\"n_outcomes\",\n","              kind=\"scatter\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"892f851a-a80b-4e9f-8a2a-b464be1bb369","metadata":{},"outputs":[],"source":["# Simulation the mean of four dice rolls\n","sample_means_1000 = []\n","for i in range(1000):\n","    sample_means_1000.append(\n","        np.random.choice(list(range(1, 7)), size=4, replace=True).mean()\n","    )\n","\n","print(sample_means_1000)\n","\n","# Approximate sampling distribution\n","plt.hist(sample_means_1000, bins=20)\n","plt.show()"]},{"cell_type":"markdown","id":"f6fb71ba-23e0-4662-bd66-f04d57d61197","metadata":{},"source":["## Standard erros and Central Limit Theorem\n","\n","### Consequences of the central limit theorem\n","\n","**Averages of independent samples have approximately normal distributions.**\n","\n","As the sample size increases,\n","- The distribution of the averages gets _closer to being normally distributed_\n","- The width of the sampling distribution gets _narrower_\n"]},{"cell_type":"code","execution_count":null,"id":"b8e8bc1d-a195-436e-8775-f1503e3c83fc","metadata":{},"outputs":[],"source":["# Population & sampling distribution standard deviations\n","coffee_ratings['total_cup_points'].std(ddof=0)\n","'''\n","- Specify ddof=0 when calling .std() on populations\n","- Specify ddof=1 when calling np.std() on samples or sampling distributions\n","'''"]},{"cell_type":"markdown","id":"dae073c3-2f30-43c2-8ba2-49a83a38a857","metadata":{},"source":["### Bootstrapping\n","**The opposite of sampling from a population**\n","\n","- Sampling: going from a population to a smaller sample\n","- Bootstrapping: building up a theoretical population from the sample\n","\n","\n","OBS: Bootstrapping use case:\n","- Develop understanding of sampling variability using a single sample\n","\n","### Bootstrapping process\n","1. Make a resample of the same size as the original sample\n","2. Calculate the statistic of interest for this bootstrap sample \n","3. Repeat steps 1 and 2 many times\n","\n","\n","**The resulting statistics are bootstrap statistics, and they form a bootstrap distribution**\n"]},{"cell_type":"code","execution_count":null,"id":"883c86cb-6ee1-435e-9898-5ca6ae2113f2","metadata":{},"outputs":[],"source":["# Bootstrapping coffee mean flavor\n","import numpy as np\n","mean_flavors_1000 = []\n","for i in range(1000):\n","    mean_flavors_1000.append(\n","        np.mean(coffee_sample.sample(frac=1, replace=True)['flavor'])\n","    )"]},{"cell_type":"markdown","id":"e6b543c9-42e2-4ea4-b05f-afaa19a77e79","metadata":{},"source":["### Interpreting the standard errors\n","- Estimated standard error → standard deviation of the bootstrap distribution for a samplestatistic\n","- Population std_dev ≈ Std_Error × √Sample_size\n"]},{"cell_type":"markdown","id":"018e3d86-9d2b-4b98-904a-0691d751f3d7","metadata":{},"source":["### Confidence intervals\n","- \"Values within one standard deviation of the mean\" includes a large number of values fromeach of these distributions\n","- We'll define a related concept called a confidence interval\n"]},{"cell_type":"code","execution_count":null,"id":"a8e66b6c-7409-4204-b0ea-3cf04348ae7d","metadata":{},"outputs":[],"source":["# Inverse cumulative distribution function\n","from scipy.stats import norm\n","norm.ppf(quantile, loc=0, scale=1)\n","\n","# Standard error method for confidence interval\n","point_estimate = np.mean(coffee_boot_distn)\n","\n","std_error = np.std(coffee_boot_distn, ddof=1)\n","\n","from scipy.stats import norm\n","lower = norm.ppf(0.025, loc=point_estimate, scale=std_error)\n","upper = norm.ppf(0.975, loc=point_estimate, scale=std_error)"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}

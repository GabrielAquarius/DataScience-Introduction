{"cells":[{"cell_type":"markdown","id":"3a6f027d-5746-4dc9-9812-2c8a364c5a48","metadata":{},"source":["# Hypothesis Testing in Python"]},{"cell_type":"markdown","id":"2c93bf83-524d-4bf9-90fc-0285ac965349","metadata":{},"source":["Hypothesis tests check if the sample statistics lie in the tails of the **null distribution**\n","- two-tailed: alternative different from null\n","- right-tailed: alternative greater than null \n","- left-tailed: alternative less than null\n","H(A): The proportion of data scientists starting programming as children is **greater than** 35%\n","This is a **right-tailed** test(A)"]},{"cell_type":"markdown","id":"92561098-d042-4074-b17f-40cdb32a2b23","metadata":{},"source":["## p-values\n","**p-values**: probability of obtaining a result, assuming the null hypothesis is true\n","1. Large p-value, large support for H(O)\n","- Statistic likely **not in** the tail of the nulldistribution\n","2. Small p-value, strong evidence against H(O)\n","- Statistic likely **in** the tail of the _null distribution_ \n","3. \"p\" in p-value → probability\n","4. \"small\" means \"close to zero\"00\n","\n","### Calculating the p-value\n","- **norm.cdf()** is normal CDF from **scipy.stats**\n","- Left-tailed test --> use **norm.cdf()**\n","- Right-tailed test --> **1 - norm.cdf()**"]},{"cell_type":"code","execution_count":null,"id":"8f3d4d95-6730-4834-8298-95c5ce15937d","metadata":{},"outputs":[],"source":["# p-value\n","from scipy.stats import norm\n","p_value = 1 - norm.cdf(z_score, loc=0, scale=1) "]},{"cell_type":"markdown","id":"3e847ce4-0cf9-4bfe-8799-a0e1e21ba344","metadata":{},"source":["## Significance level\n","The significance level of a hypothesis test (α) is the threshold point for \"beyond a reasonable doubt\"\n","- Common values of α are 0.2, 0.1, 0.05, and 0.01\n","- If p≤α, reject H, else fail to reject H\n","- α should be set prior to conducting the hypothesis test00\n"]},{"cell_type":"code","execution_count":null,"id":"ccd971a0-ea99-4dd3-9214-a59fb026db66","metadata":{},"outputs":[],"source":["# Making a decision\n","alpha = 0.05\n","print(p_value <= alpha)"]},{"cell_type":"markdown","id":"fef1686f-d8b2-4085-a37e-0a65783640f0","metadata":{},"source":["## Confidence intervals\n","For a significance level of α, it's common to choose a confidence interval level of 1 - α\n","- α=0.05 → 95% confidence interval"]},{"cell_type":"code","execution_count":null,"id":"e444d545-921f-403d-9dae-39ac4a5ce0a6","metadata":{},"outputs":[],"source":["# Confidence intervals\n","import numpy as np\n","lower = np.quantile(first_code_boot_distn, 0.025)\n","upper = np.quantile(first_code_boot_distn, 0.975)\n","print((lower, upper))"]},{"cell_type":"markdown","id":"1d023faa-cadf-44cf-b30e-b70fb47dae44","metadata":{},"source":["## Performing t-tests"]},{"cell_type":"code","execution_count":null,"id":"0b323817-fd36-412d-93ca-136e30381442","metadata":{},"outputs":[],"source":["# Calculations assuming the null hypothesis is true\n","xbar = stack_overflow.groupby('age_first_code_cut')['converted_comp'].mean()\n","s = stack_overflow.groupby('age_first_code_cut')['converted_comp'].std()\n","n = stack_overflow.groupby('age_first_code_cut')['converted_comp'].count()\n","\n","# Calculating the test statistic\n","import numpy as np\n","numerator = xbar_child - xbar_adult\n","denominator = np.sqrt(s_child ** 2 / n_child + s_adult ** 2 / n_adult)\n","t_stat = numerator / denominator\n","\n","# Calculating the degrees of freedom\n","degrees_of_freedom = n_child + n_adult - 2\n","\n","# Calculating p-values from t-values\n","from scipy.stats import t\n","1 - t.cdf(t_stat, df=degrees_of_freedom)"]},{"cell_type":"markdown","id":"9c679908-9356-4730-8b9f-98b87672e348","metadata":{},"source":["## Paired t-tests"]},{"cell_type":"code","execution_count":null,"id":"14e636a6-357a-417b-b120-363bc787b485","metadata":{},"outputs":[],"source":["# Testing differences between two means using ttest()\n","import pingouin\n","pingouin.ttest(x=sample_data['diff'],\n","               y=0,\n","               alternative=\"less\")\n","\n","# ttest() with paired=True\n","pingouin.ttest(x=sample_data['repub_percent_08'],               \n","               y=sample_data['repub_percent_12'],\n","               paired=True,\n","               alternative=\"less\")\n","\n","# Unpaired ttest()\n","pingouin.ttest(x=sample_data['repub_percent_08'],               \n","               y=sample_data['repub_percent_12'],\n","               paired=False, # The default\n","               alternative=\"less\")"]},{"cell_type":"markdown","id":"f7a6eac6-4fb9-47aa-bd34-cc77ec98c1e6","metadata":{},"source":["## ANOVA tests\n","\n","### Methods without example\n","**Method used for testing and adjustment of pvalues.**\n","- **'none'**: no correction (default)\n","- **'bonf'**: one-step Bonferroni correction\n","- **'sidak'**: one-step Sidak correction\n","- **'holm'**: step-down method using Bonferroni adjustments\n","- **'fdr_bh'**: Benjamini/Hochberg FDR correction\n","- **'fdr_by'**: Benjamini/Yekutieli FDR correction\n"]},{"cell_type":"code","execution_count":null,"id":"7aa7759d-0900-4e71-ae9b-bd994e896990","metadata":{},"outputs":[],"source":["# Analysis of variance (ANOVA)\n","\n","# A test for differences between groups\n","alpha = 0.2\n","pingouin.anova(data=stack_overflow,\n","                          dv=\"converted_comp\",\n","                          between=\"job_sat\")\n","# 0.001315 < α\n","# At least two categories have significantly different compensation\n","\n","# pairwise_tests()\n","pingouin.pairwise_tests(data=stack_overflow,                         \n","                        dv=\"converted_comp\",                         \n","                        between=\"job_sat\",\n","                        padjust=\"none\")\n","# Bonferroni correction\n","pingouin.pairwise_tests(data=stack_overflow,                         \n","                        dv=\"converted_comp\",\n","                        between=\"job_sat\",\n","                        padjust=\"bonf\")"]},{"cell_type":"markdown","id":"c1ab7bf8-d47e-4586-bab8-625659f07d3d","metadata":{},"source":["## Proportion tests"]},{"cell_type":"markdown","id":"0d7c7ec7-4ae1-4a6e-b215-842798b34f4f","metadata":{},"source":["### One-sample proportion tests"]},{"cell_type":"code","execution_count":null,"id":"280ff094-e224-44a3-a1f4-ccd39fcdbd2b","metadata":{},"outputs":[],"source":["# Caculating the z-score\n","import numpy as np\n","numerator = p_hat - p_0\n","denominator = np.sqrt(p_0 * (1 - p_0) / n)\n","z_score = numerator / denominator\n","\n","# Calculating the p_value\n","from scipy.stats import norm\n","\n","# Left-tailed (\"less than\")\n","p_value = norm.cdf(z_score)\n","\n","# Right-tailed (\"greater than\")\n","p_value = 1 - norm.cdf(z_score)\n","\n","# Two-tailed (\"not equal\")\n","p_value = norm.cdf(-z_score) + 1 - norm.cdf(z_score)\n","\n","# If pdf was symmetric\n","p_value = 2 * (1 - norm.cdf(z_score))"]},{"cell_type":"markdown","id":"e1c87e33-7a5c-49a6-902c-ce2b58dd9fed","metadata":{},"source":["### Two-sample proportion tests"]},{"cell_type":"code","execution_count":null,"id":"9f072313-8a60-4a37-9184-e45d198053f6","metadata":{},"outputs":[],"source":["# Getting the numbers for the z-score\n","p_hat = (n_at_least_30 * p_hat_at_least_30 + n_under_30 * p_hat_under_30) /         (n_at_least_30 + n_under_30)\n","\n","std_error = np.sqrt(p_hat * (1-p_hat) / n_at_least_30) + (p_hat * 1-p_hat) / n_under_30))\n","\n","z_score = (p_hat_at_least_30 - p_hat_under_30) / std_error\n","\n","print(z_score)\n","\n","# Proportion tests using proportions_ztest()\n","n_hobbyists = np.array([812, 1021])\n","n_rows = np.array([812 + 238, 1021 + 190])\n","from statsmodels.stats.proportion import proportions_ztest\n","z_score, p_value = proportions_ztest(count=n_hobbyists, nobs=n_rows,                                     alternative=\"two-sided\")"]},{"cell_type":"markdown","id":"3fbe0689-8e67-4c46-874f-4e9576eefe6d","metadata":{},"source":["### Chi-square test of independence"]},{"cell_type":"code","execution_count":null,"id":"1293b89a-9a1b-431a-b53b-f4dbdcf7bffd","metadata":{},"outputs":[],"source":["# Test for independence of variables \n","import pingouin\n","expected, observed, stats = pingouin.chi2_independence(data=stack_overflow, x='hobbyist', y='age_cat', correction=False)\n","\n","print(stats)"]},{"cell_type":"markdown","id":"e43e267e-8e19-4879-a93b-d55cc97efec0","metadata":{},"source":["### Chi-square gooodness of fit tests\n","**Purple links**\n","- How do you feel when you discover that you've already visited the top resource?"]},{"cell_type":"code","execution_count":null,"id":"dcba6a5b-2560-4cc6-8e9f-d50dadb7219a","metadata":{},"outputs":[],"source":["# Purple links\n","purple_link_counts = stack_overflow['purple_link'].value_counts()\n","\n","purple_link_counts = purple_link_counts.rename_axis('purple_link')\\                                                      .reset_index(name='n')\\                                                            .sort_values('purple_link')\n","\n","# Declaring the hypotheses\n","hypothesized = pd.DataFrame({'purple_link': ['Amused', 'Annoyed', 'Hello, old friend', 'Indifferent'], 'prop': [1/6, 1/6, 1/2, 1/6]})\n","\n","# Hypothesized counts by category\n","n_total = len(stack_overflow)\n","hypothesized[\"n\"] = hypothesized[\"prop\"] * n_total\n","\n","# Visualizing counts\n","import matplotlib.pyplot as plt\n","\n","plt.bar(purple_link_counts['purple_link'], purple_link_counts['n'],                       color='red', label='Observed')\n","\n","plt.bar(hypothesized['purple_link'], hypothesized['n'], alpha=0.5,                         color='blue', label='Hypothesized')\n","\n","plt.legend()\n","plt.show()\n","\n","# chi-square goodness of fit test\n","from scipy.stats import chisquare\n","\n","chisquare(f_obs=purple_link_counts['n'], f_exp=hypothesized['n'])"]},{"cell_type":"markdown","id":"8e213b21-1501-474f-993c-178c50d8f892","metadata":{},"source":["## Non-Parametric Tests"]},{"cell_type":"code","execution_count":null,"id":"38228c50-5201-48bb-95a5-a6dbc4b0fd52","metadata":{},"outputs":[],"source":["# Results with pingouin.ttest()\n","alpha = 0.01\n","import pingouin\n","pingouin.ttest(x=repub_votes_potus_08_12_small['repub_percent_08'],                              y=repub_votes_potus_08_12_small['repub_percent_12'],\n","               paired=True,\n","               alternative=\"less\")\n","\n","# Non-parametric tests\n","x = [1, 15, 3, 10, 6]\n","from scipy.stats import rankdata\n","rankdata(x)\n","\n","# Wilcoxon-signed rank test\n","# Step 1\n","repub_votes_small['diff'] = repub_votes_small['repub_percent_08'] - repub_votes_small['repub_percent_12']\n","\n","# Step 2\n","repub_votes_small['abs_diff'] = repub_votes_small['diff'].abs()\n","\n","# Step 3 \n","from scipy.stats import rankdata\n","repub_votes_small['rank_abs_diff'] = rankdata(repub_votes_small['abs_diff'])\n","\n","# Step 4\n","T_minus = 1 + 4 + 5 + 2 + 3\n","T_plus = 0\n","W = np.min([T_minus, T_plus])\n","\n","# Implementation with pingouin.wilcoxon()\n","alpha = 0.01\n","pingouin.wilcoxon(x=repub_votes_potus_08_12_small['repub_percent_08'],                               y=repub_votes_potus_08_12_small['repub_percent_12'],\n","                  alternative=\"less\")"]},{"cell_type":"markdown","id":"98fd47e4-2d48-4fdf-8a93-0db090b44c94","metadata":{},"source":["### Non-parametric ANOVA and unpaired t-tests"]},{"cell_type":"code","execution_count":null,"id":"1775a1ea-3c3e-44fb-8af8-a634ba6194b1","metadata":{},"outputs":[],"source":["# Wilcoxon-Mann_whitney test setup\n","age_vs_comp = stack_overflow[['converted_comp', 'age_first_code_cut']]\n","age_vs_comp_wide = age_vs_comp.pivot(columns='age_first_code_cut',\n","                                     values='converted_comp')\n","\n","# Wilcoxon-Mann_whitney test\n","alpha=0.01\n","import pingouin\n","pingouin.mwu(x=age_vs_comp_wide['child'],              \n","             y=age_vs_comp_wide['adult'],\n","             alternative='greater')\n","\n","# Kruskal-Wallis test\n","alpha=0.01\n","pingouin.kruskal(data=stack_overflow,\n","                 dv='converted_comp',\n","                 between='job_sat')"]},{"cell_type":"markdown","id":"44c31da2","metadata":{},"source":["# Preprocessing for Machine Learning in Python"]},{"cell_type":"code","execution_count":null,"id":"31841016","metadata":{},"outputs":[],"source":["# Removing missing data\n","'''\n","df.drop([], axis=0 or 1)\n","df.dropna(subset=[])\n","# specifies the columns that will have their rows deleted \n","'''\n","\n","# Checking column type\n","'''\n","df.info() or df.dtypes\n","'''\n","\n","# Converting column type\n","'''\n","df['column_1'] = df[column_1].astype('float')\n","'''"]},{"cell_type":"code","execution_count":null,"id":"b65605b5","metadata":{},"outputs":[],"source":["# Solving imbalanced classes with strafied sampling\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, \n","                                                    stratify=y,\n","                                                    test_size=0.2,\n","                                                    random_state=42)"]},{"cell_type":"markdown","id":"5522bd64","metadata":{},"source":["### Log normalization\n","- Useful for features with high variance\n","- Applies lofarithm transformation\n","- Natural log using the constant *e*\n","- Capture relative changes, the magnitude of change, and keeps everything positive"]},{"cell_type":"code","execution_count":null,"id":"01df0600","metadata":{},"outputs":[],"source":["# Log nomalization in Python\n","import pandas as pd\n","df = pd.DataFrame({\n","    'col1':[1.00, 1.20, 0.75, 1.60],\n","    'col2':[3.0, 45.5, 28.0, 100.0]\n","})\n","\n","print(df.var())"]},{"cell_type":"code","execution_count":null,"id":"cf8cb7fd","metadata":{},"outputs":[],"source":["import numpy as np\n","df['log_2'] = np.log(df['col2'])\n","print(df)\n","print(df[['col1', 'log_2']].var())"]},{"cell_type":"markdown","id":"d73f3864","metadata":{},"source":["### Feature Scaling\n","- Features on different scales\n","- Model with linear characteristics\n","- Center features around 0 and transform to variance of 1\n","- Transforms to approximately normal distribution"]},{"cell_type":"code","execution_count":null,"id":"69d6a841","metadata":{},"outputs":[],"source":["# Scale data\n","df = pd.DataFrame({\n","    'col1':[1.00, 1.20, 0.75, 1.60],\n","    'col2':[48.0, 45.5, 46.2, 50.0],\n","    'col3':[100.0, 101.3, 103.5, 104.0]\n","})\n","\n","print(df.var())"]},{"cell_type":"markdown","id":"da0eae05","metadata":{},"source":["#### Data leakage: non-training data is used to train the model"]},{"cell_type":"code","execution_count":null,"id":"717b4e6b","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","df_scaled = pd.DataFrame(scaler.fit_transform(df),\n","                         columns=df.columns)\n","print(df_scaled)\n","print(df_scaled.var())"]},{"cell_type":"markdown","id":"ae6e3b2b","metadata":{},"source":["### Feature Engineering\n","#### Feature engeneering: Creation of new features from existing ones\n","- Improve performance\n","- Insight relationships between features\n","- Need to understand the data first\n","- Highly dataset-dependent"]},{"cell_type":"code","execution_count":null,"id":"2c7ce8ed","metadata":{},"outputs":[],"source":["# Encoding categorical variables\n","users = pd.DataFrame({\n","    'subscribed':['y', 'n', 'n', 'y']\n","    'fav_color':['blue', 'green', 'orange', 'green']\n","})\n","\n","users['sun_enc'] = users['subscribed'].apply(lambda val: 1 if val=='y' else 0)\n","\n","\n","# Encoding binary variables with scikit-learn\n","\n","# Label encoding\n","from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","users['sub_enc_le'] = le.fit_transform(users['subscribed'])\n","print(users['subscribed', 'sub_enc_le'])\n","\n","# One-hot encoding\n","print(pd.get_dummies(users['fav_color']))"]},{"cell_type":"markdown","id":"cd95f74b","metadata":{},"source":["### Engineering numerical features"]},{"cell_type":"code","execution_count":null,"id":"0afa27c7","metadata":{},"outputs":[],"source":["# Descriptive Statistics\n","temps = pd.DataFrame({\n","    'city':['NYC', 'SF', 'LA', 'Boston'],\n","    'day1':[68.3, 75.1, 80.3, 63.0],\n","    'day2':[67.9, 75.5, 84.0, 61.0],\n","    'day3':[67.8, 74.9, 81.3, 61.2]\n","})\n","\n","temps['mean'] = temps.loc[:, 'day1':'day3'].mean(axis=1)\n","print(temps)"]},{"cell_type":"code","execution_count":null,"id":"6fd93161","metadata":{},"outputs":[],"source":["# Dates\n","purchases = pd.DataFrame({\n","    'date':['July 30 2011', 'February 01 2011', 'January 29 2011', 'March 31 2012', 'February 5 2011'],\n","    'purchase':['$45.08', '$19.48', '$76.09', '$32.61', '$75.98']\n","})\n","\n","purchases['date_converted'] = pd.to_datetime(purchases['date'])\n","purchases['month'] = purchases['date_converted'].dt.month\n","print(purchases)"]},{"cell_type":"markdown","id":"65fe2068","metadata":{},"source":["### Engineering text features\n","#### Regular expressions: code to identify patterns"]},{"cell_type":"code","execution_count":null,"id":"c6305ca0","metadata":{},"outputs":[],"source":["# Extraction\n","import re\n","my_string = 'temperature:75.6 F'\n","temp = re.search('\\d+\\.\\d+', my_string)\n","\n","print(float(temp.group(0)))"]},{"cell_type":"markdown","id":"ea4197f3","metadata":{},"source":["### Vectorizing text\n","#### TF/IDF: Vectorizes words based upon importance\n","- TF = Term Frequency\n","- IDF = Inverse Document Frequency"]},{"cell_type":"code","execution_count":null,"id":"334b3c92","metadata":{},"outputs":[],"source":["# Vectorizing text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vec = TfidfVectorizer()\n","text_tfidf = tfidf_vec.fit_transform(documents)"]},{"cell_type":"markdown","id":"a3f3b59b","metadata":{},"source":["### Feature Selection\n","- Reduce noise\n","- Features with strongly statistically correlated\n","- Reduce overall variance\n","\n","### Redudant Features\n","- Remove noisy features\n","- Remove correlated features\n","- Remove duplicated features\n","\n","### Correlated Features\n","- Statiscally correlated: features move together directionally\n","- Linear models assume feature independence\n","- Pearson's correlation coefficient"]},{"cell_type":"code","execution_count":null,"id":"834f6967","metadata":{},"outputs":[],"source":["# Correlated features\n","df = pd.DataFrame({\n","    'A':[3.06, 2.76, 3.24],\n","    'B':[3.92, 3.40, 3.17],\n","    'C':[1.04, 1.05, 1.03]\n","})\n","\n","print(df.corr())"]},{"cell_type":"markdown","id":"656fe5bf","metadata":{},"source":[" ### Selecting features using text vectors"]},{"cell_type":"code","execution_count":null,"id":"4b236513","metadata":{},"outputs":[],"source":["# Looking at word weights\n","print(tfidf_vec.vocabulary_)\n","\n","print(text_tfidf[3].data)\n","\n","print(text_tfidf[3].indices)"]},{"cell_type":"code","execution_count":null,"id":"d55a463b","metadata":{},"outputs":[],"source":["vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n","print(vocab)"]},{"cell_type":"code","execution_count":null,"id":"cc56da60","metadata":{},"outputs":[],"source":["def return_weights(vocab, vector, vector_index):\n","    zipped = dict(zip(vector[vector_index].indices,\n","                      vector[vector_index].data))\n","    \n","    return {vocab[i]:zipped[i] for i in vector[vector_index].indices}\n","\n","print(return_weights(vocab, text_tfidf, 3))"]},{"cell_type":"markdown","id":"7e7a4d12","metadata":{},"source":["### Dimensionality reduction\n","- Unsupervised learning method\n","- Combines/decomposes a feature space\n","- Feature extraction - here we'll use to reduce our feature space\n","\n","### PCA\n","- Principal component analysis\n","- Linear transformation to uncorrelated space\n","- Captures as much variance as possible in each component\n","\n","#### PCA caveats\n","- Difficult to interpret components\n","- End of preprocessing journey"]},{"cell_type":"code","execution_count":null,"id":"0b91f3c3","metadata":{},"outputs":[],"source":["# PCA in scikit-learn\n","from sklearn.decomposition import PCA\n","pca = PCA()\n","df_pca = pca.fit_transform(df)\n","\n","print(df_pca)\n","print(pca.explained_variance_ratio_)"]},{"cell_type":"markdown","id":"47f1afda-7404-4a46-a724-79d05a864a5a","metadata":{},"source":["# Supervised Learning with Scikit-learn"]},{"cell_type":"markdown","id":"7da0c0c6-d909-4ab1-9b25-9a7098c2b3e6","metadata":{},"source":["### Naming conventions\n","- Feature = predictor variable = independent variable\n","- Target variable = response variable = dependent variable "]},{"cell_type":"code","execution_count":null,"id":"c80e461d-3d13-4d15-b0e7-39e24a6b8561","metadata":{},"outputs":[],"source":["# scikit-learn syntax\n","from sklearn.module import Model\n","\n","model = Model()\n","model.fit(X, y)\n","\n","predictions = model.predict(X_new)\n","\n","print(predictions)"]},{"cell_type":"markdown","id":"5af83425-cbb5-4284-971a-6ce0158a77df","metadata":{},"source":["### Classifying labels of unseen data\n","1. Build a model\n","2. Model learns from the labeled data we pass to it \n","3. Pass unlabeled data to the model as input \n","4. Model predicts the labels of the unseen data \n","- Labeled data = training data"]},{"cell_type":"code","execution_count":null,"id":"6244e2dc-2961-4294-979a-5bc19764dba1","metadata":{},"outputs":[],"source":["# k-Nearest Neighbors\n","from sklearn.neighbors import KNeighborsClassifier\n","X = churn_df[[\"total_day_charge\", \"total_eve_charge\"]].values\n","y = churn_df[\"churn\"].values\n","\n","print(X.shape, y.shape)\n","\n","# Using scikit-learn to fit a classifier\n","knn = KNeighborsClassifier(n_neighbors=15)\n","knn.fit(X, y)"]},{"cell_type":"code","execution_count":null,"id":"b82ca31b-a61f-48fc-9277-efef7abfeaef","metadata":{},"outputs":[],"source":["# Prediciting on unlabeled data\n","X_new = np.array([[56.8, 17.5],\n","                  [24.4, 24.1],\n","                  [50.1, 10.9]])\n","print(X_new.shape)\n","\n","predictions = knn.predict(X_new)\n","print('Predictions: {}'.format(predictions))"]},{"cell_type":"markdown","id":"4f89aa4c-b8bb-417a-9f7d-601ce6d0fae3","metadata":{},"source":["### Measuring Model Performance\n","- Larger k = less complex model = can cause underfitting\n","- Smaller k = more complex model = can lead to overfitting"]},{"cell_type":"code","execution_count":null,"id":"5305bdd9-1315-4639-8d91-26e2bbbac3c3","metadata":{},"outputs":[],"source":["# Train/test split\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                    random_state=21,\n","                                                    stratify=y)\n","knn = KNeighborsClassifier(n_neighbors=6)\n","\n","knn.fit(X_train, y_train)\n","\n","print(knn.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"2737e8a3-8114-4a6b-95e9-09e14b81363d","metadata":{},"outputs":[],"source":["# Model complexity and over/underfitting\n","train_accuracies = {}\n","test_accuracies = {}\n","neighbors = np.arange(1, 26)\n","for neighbor in neighbors:\n","    knn = KNeighborsClassifier(n_neighbors=neighbor)\n","    knn.fit(X_train, y_train)\n","    train_accuracies[neighbor] = knn.score(X_train, y_train)\n","    test_accuracies[neighbor] = knn.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"id":"e3456690-f1ee-4746-aed1-c0a10cbed0f9","metadata":{},"outputs":[],"source":["# Plotting our results\n","plt.figure(figsize=(8, 6))\n","plt.title(\"KNN: Varying Number of Neighbors\")\n","plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n","plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n","plt.legend()\n","plt.xlabel(\"Number of Neighbors\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()\n"]},{"cell_type":"markdown","id":"1f610dc6-390a-4c90-b51d-36068a4fc0d3","metadata":{},"source":["## Introduction to Regression"]},{"cell_type":"code","execution_count":null,"id":"6c8bba72-d4d0-48b9-8ccb-cb6c4b3adbad","metadata":{},"outputs":[],"source":["# Fitting a regression model\n","from sklearn.linear_model import LinearRegression\n","reg = LinearRegression()\n","reg.fit(X_bmi, y)\n","predictions = reg.predict(X_bmi)\n","\n","# Plotting the results\n","plt.scatter(X_bmi, y)\n","plt.plot(X_bmi, predictions)\n","plt.ylabel('Blood Glucose (mg/dl)')\n","plt.xlabel('Body Mass Index')\n","plt.show()"]},{"cell_type":"markdown","id":"3c4d355e-bebb-40a8-a0cb-b3fcf4156710","metadata":{},"source":["## R-squared, MSE and RMSE"]},{"cell_type":"code","execution_count":null,"id":"8855963d-3462-4174-931b-98979d592c19","metadata":{},"outputs":[],"source":["# Linear Regression using all faetures\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n","                                                    random_state=42)\n","reg_all = LinearRegression()\n","reg_all.fit(X_train, y_train)\n","y_pred = reg_all.predict(X_test)\n","\n","# R-squared in scikit-learn\n","print(reg_all.score(X_test, y_test))\n","\n","# RMSE in scikit-learn\n","from sklearn.metrics import mean_squared_error\n","print(mean_squared_error(y_test, y_pred, squared=False))"]},{"cell_type":"markdown","id":"ffdca269-cc39-44b4-a48b-f2822c571aec","metadata":{},"source":["## Cross-validation"]},{"cell_type":"code","execution_count":null,"id":"f5d09a87-b95d-407b-8776-ac773b366e8f","metadata":{},"outputs":[],"source":["# Cross-validation in scikit-learn\n","from sklearn.model_selection import cross_val_score, KFold\n","kf = KFold(n_splits=6, shuffle=True, random_state=42)\n","reg = LinearRegression()\n","cv_results = cross_val_score(reg, X, y, cv=kf)\n","\n","# Evaluating cross-validation performance\n","print(cv_results)\n","print(np.mean(cv_results), np.std(cv_results))\n","print(np.quantile(cv_results, [0.025, 0.975]))"]},{"cell_type":"markdown","id":"1f547c60-0c5a-44b5-9db1-36bdf506ef38","metadata":{},"source":["## Regularized regression"]},{"cell_type":"code","execution_count":null,"id":"0e095a46-c9a6-4bda-a770-581b33b8b03e","metadata":{},"outputs":[],"source":["# Ridge regression in scikit-learn\n","from sklearn.linear_model import Ridge\n","scores = []\n","for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:\n","    ridge = Ridge(alpha=alpha)\n","    ridge.fit(X_train, y_train)\n","    y_pred = ridge.predict(X_test)\n","    scores.append(ridge.score(X_test, y_test))\n","print(scores)"]},{"cell_type":"code","execution_count":null,"id":"ea39692d-3c86-468f-99c5-76286085a42f","metadata":{},"outputs":[],"source":["# Lasso for feature selection in scikit-learn\n","from sklearn.linear_model import Lasso\n","X = diabetes_df.drop(\"glucose\", axis=1).values\n","y = diabetes_df[\"glucose\"].values\n","\n","names = diabetes_df.drop(\"glucose\", axis=1).columns\n","lasso = Lasso(alpha=0.1)\n","lasso_coef = lasso.fit(X, y).coef_\n","\n","# Plot the results\n","plt.bar(names, lasso_coef)\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","id":"2ad890d4-0e98-4ed0-81ea-f3e9644b1962","metadata":{},"source":["## Confusion matrix in scikit-learn"]},{"cell_type":"code","execution_count":null,"id":"d4a0c433-49ca-4cf4-bee4-4ca3253b6cd0","metadata":{},"outputs":[],"source":["# Confusion matrix and Classification report\n","from sklearn.metrics import classification_report, confusion_matrix\n","knn = KNeighborsClassifier(n_neighbors=7)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n","                                                    random_state=42)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","id":"1b2513f5-e721-4053-9254-a3ffb612d813","metadata":{},"source":["## Logistic regression for binary classification\n","1. Logistic regression is used for classification problems\n","2. Logistic regression outputs probabilities\n","3. If the probability, p > 0.5:\n","- The datais labeled 1\n","4. If the probability, p < 0.5:\n","- The data is labeled 0"]},{"cell_type":"code","execution_count":null,"id":"c9241a99-49ae-4eb7-bb8a-ea32cb9cf09d","metadata":{},"outputs":[],"source":["# Logistic Regression\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n","                                                    random_state=42)\n","logreg.fit(X_train, y_train)\n","y_pred = logreg.predict(X_test)\n","\n","# Predicting probabilities\n","y_pred_probs = logreg.predict_proba(X_test)[:, 1]\n","print(y_pred_probs[0])"]},{"cell_type":"markdown","id":"787ecd02-994d-49d8-b918-f955aa968b6b","metadata":{},"source":["## Probability thresholds\n","1. By default, logistic regression threshold = 0.5\n","2. Not specific to logistic regression\n","- KNN classifiers also have thresholds"]},{"cell_type":"code","execution_count":null,"id":"8529cb20-a2be-4dd9-b5cc-161974cd4631","metadata":{},"outputs":[],"source":["# Plotting the ROC curve\n","from sklearn.metrics import roc_curve\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr, tpr)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Logistic Regression ROC Curve')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"db9bb7c9-082d-4141-813a-119d49f7f26a","metadata":{},"outputs":[],"source":["# ROC AUC in scikit-learn\n","from sklearn.metrics import roc_auc_score\n","print(roc_auc_score(y_test, y_pred_probs))"]},{"cell_type":"markdown","id":"c230868b-f78a-4cdb-b117-900d344ccd24","metadata":{},"source":["## Hyperparameter tuning\n","- Ridge/lasso regression: Choosing **alpha**\n","- KNN: Choosing **n_neighbors**\n","- Hyperparameters: Parameters we specify before fitting the model\n","\n","### Choosing the correct hyperparameters\n","1. Try lots of different hyperparameter values\n","2. Fit all of them separately\n","3. See how well they perform\n","4. Choose the best performing values\n","\n","### Observations:\n","- This is called **hyperparameter tuning**\n","- It is essential to use cross-validation to avoid overfitting to the test set\n","- We can still split the data and perform cross-validation on the training set\n","- We withhold the test set for final evaluation"]},{"cell_type":"code","execution_count":null,"id":"6be6608e-4189-4a58-9a20-80a663c8b63e","metadata":{},"outputs":[],"source":["# GridSearhCV\n","from sklearn.model_selection import GridSearchCV\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","param_grid = {\"alpha\": np.arange(0.0001, 1, 10),\n","              \"solver\": [\"sag\", \"lsqr\"]}\n","ridge = Ridge()\n","ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)\n","ridge_cv.fit(X_train, y_train)\n","print(ridge_cv.best_params_, ridge_cv.best_score_)"]},{"cell_type":"code","execution_count":null,"id":"32f2117d-a580-4948-8c21-6b637e36f759","metadata":{},"outputs":[],"source":["# RandomizedSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","param_grid = {'alpha': np.arange(0.0001, 1, 10),\n","              \"solver\": ['sag', 'lsqr']}\n","ridge = Ridge()\n","ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)\n","ridge_cv.fit(X_train, y_train)\n","print(ridge_cv.best_params_, ridge_cv.best_score_)"]},{"cell_type":"markdown","id":"e3dd4b0d-bae5-42e8-abb9-56738ff87478","metadata":{},"source":["## Preprocessing data"]},{"cell_type":"code","execution_count":null,"id":"730f47d6-94e0-4cce-b083-9163d801208d","metadata":{},"outputs":[],"source":["# Encoding dummy variables\n","import pandas as pd\n","music_df = pd.read_csv('music.csv')\n","\n","music_dummies = pd.get_dummies(music_df[\"genre\"], drop_first=True)\n","\n","print(music_dummies.head())\n","\n","music_dummies = pd.concat([music_df, music_dummies], axis=1)music_dummies = music_dummies.drop(\"genre\", axis=1)\n","\n","# If the DataFrame only has one categorical feature, we can pass the entire DataFrame:\n","# music_dummies = pd.get_dummies(music_df, drop_first=True)\n","# print(music_dummies.columns)"]},{"cell_type":"code","execution_count":null,"id":"9d485d8a-18b9-41d7-ab78-0db3fba82f20","metadata":{},"outputs":[],"source":["# Linear regression with dummy variables\n","from sklearn.model_selection import cross_val_score, KFoldfrom sklearn.linear_model import LinearRegression\n","X = music_dummies.drop(\"popularity\", axis=1).values\n","y = music_dummies[\"popularity\"].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                    random_state=42)\n","kf = KFold(n_splits=5, \n","           shuffle=True, \n","           random_state=42)\n","\n","linreg = LinearRegression()\n","\n","linreg_cv = cross_val_score(linreg, X_train, y_train, cv=kf,\n","                            scoring=\"neg_mean_squared_error\")\n","print(np.sqrt(-linreg_cv))"]},{"cell_type":"markdown","id":"8c1ef0dd-9c07-47cb-99c9-e0e756066206","metadata":{},"source":["## Handling missing data"]},{"cell_type":"code","execution_count":null,"id":"65a58035-aa1e-4da6-81a2-df132b2aea5b","metadata":{},"outputs":[],"source":["# Imputation with scikit-learn\n","from sklearn.impute import SimpleImputer\n","X_cat = music_df[\"genre\"].values.reshape(-1, 1)\n","X_num = music_df.drop([\"genre\", \"popularity\"], axis=1).values\n","y = music_df[\"popularity\"].values\n","\n","X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_cat, y,\n","test_size=0.2, random_state=12)\n","\n","X_train_num, X_test_num, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=12)\n","\n","imp_cat = SimpleImputer(strategy=\"most_frequent\")\n","\n","X_train_cat = imp_cat.fit_transform(X_train_cat)\n","X_test_cat = imp_cat.transform(X_test_cat)"]},{"cell_type":"code","execution_count":null,"id":"096516f2-abbc-4908-911a-fd6c9c221a33","metadata":{},"outputs":[],"source":["# Imputing within a pipeline\n","from sklearn.pipeline import Pipeline\n","music_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \n","                                   \"tempo\"])\n","\n","music_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n","\n","X = music_df.drop(\"genre\", axis=1).values\n","y = music_df[\"genre\"].values\n","\n","steps = [(\"imputation\", SimpleImputer()),\n","         (\"logistic_regression\", LogisticRegression())]\n","\n","pipeline = Pipeline(steps)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                    random_state=42)\n","pipeline.fit(X_train, y_train)\n","pipeline.score(X_test, y_test)"]},{"cell_type":"markdown","id":"79f59672-3d12-443c-9793-b28ae0e85c91","metadata":{},"source":["## Centering and scaling"]},{"cell_type":"code","execution_count":null,"id":"7a380530-3c9e-4681-a73d-ca1b795421ee","metadata":{},"outputs":[],"source":["# Scaling in scikit-learn\n","from sklearn.preprocessing import StandardScaler\n","X = music_df.drop(\"genre\", axis=1).values\n","y = music_df[\"genre\"].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                    random_state=42)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","print(np.mean(X), np.std(X))\n","print(np.mean(X_train_scaled), np.std(X_train_scaled))"]},{"cell_type":"code","execution_count":null,"id":"7eda8162-48af-41de-9b1b-860ce77a3eb3","metadata":{},"outputs":[],"source":["# Scaling in a pipeline\n","steps = [('scaler', StandardScaler()),\n","         ('knn', KNeighborsClassifier(n_neighbors=6))]\n","pipeline = Pipeline(steps)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                    random_state=21)\n","knn_scaled = pipeline.fit(X_train, y_train)\n","y_pred = knn_scaled.predict(X_test)\n","\n","print(knn_scaled.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"0d73cd26-53fc-4414-a973-772bab7b2c1f","metadata":{},"outputs":[],"source":["# Comparing performance using unsacaled data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                    random_state=21)\n","knn_unscaled = KNeighborsClassifier(n_neighbors=6).fit(X_train, y_train)\n","print(knn_unscaled.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"f4ec9fb0-92c2-47b4-a222-5b22ad49a5f9","metadata":{},"outputs":[],"source":["# Cross-validation and scaling in a pipeline\n","from sklearn.model_selection import GridSearchCV\n","steps = [('scaler', StandardScaler()),\n","         ('knn', KNeighborsClassifier())]\n","pipeline = Pipeline(steps)\n","parameters = {\"knn__n_neighbors\": np.arange(1, 50)}\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                    random_state=21)\n","cv = GridSearchCV(pipeline, param_grid=parameters)\n","cv.fit(X_train, y_train)\n","y_pred = cv.predict(X_test)"]},{"cell_type":"markdown","id":"ba94cd61-46b7-4b40-a61d-4f2e5063f755","metadata":{},"source":["## Evaluating multiple models"]},{"cell_type":"code","execution_count":null,"id":"61d04438-ab38-4755-9778-5d4a2e4ca6e3","metadata":{},"outputs":[],"source":["# Evaluating calssification models\n","models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree\": DecisionTreeClassifier()}\n","results = []\n","for model in models.values():\n","    kf = KFold(n_splits=6, random_state=42, shuffle=True)\n","    cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n","    results.append(cv_results)\n","\n","plt.boxplot(results, labels=models.keys())\n","plt.show()\n"]},{"cell_type":"markdown","id":"de9625bd-5fe1-4c00-a71a-a2c0e8f12aa0","metadata":{},"source":["# Unsupervised Learning"]},{"cell_type":"markdown","id":"5841d3a8-7c86-4bd3-ad02-92cc4e61c7af","metadata":{},"source":["## K-means clustering\n","- Finds clusters of samples\n","- Number of clusters must be specified\n","\n","### Cluster labels for new samples\n","- New samples can be assigned to existing clusters\n","- k-means remembers the mean of each cluster(the \"centroids\")\n","- Finds the nearest centroid to each new sample"]},{"cell_type":"code","execution_count":null,"id":"6818f491-dbab-4842-8e5c-7cab7c4201db","metadata":{},"outputs":[],"source":["# k-means clustering\n","from sklearn.cluster import KMeans\n","model = KMeans(n_clusters=3)\n","model.fit(samples)\n","labels = model.predict(samples)\n","print(labels)"]},{"cell_type":"code","execution_count":null,"id":"c05b347e-58be-4e95-b291-a4bb1896bc9a","metadata":{},"outputs":[],"source":["# Visualizing with scatter plot\n","import matplotlib.pyplot as plt\n","xs = samples[:,0]\n","ys = samples[:,2]\n","plt.scatter(xs, ys, c=labels)\n","plt.show()"]},{"cell_type":"markdown","id":"8e731ba6-4e4f-4dc7-9156-4dd8bb4a88af","metadata":{},"source":["## Evaluating a clustering"]},{"cell_type":"code","execution_count":null,"id":"4f5e26be-6af3-427a-a2f6-ac008a67a492","metadata":{},"outputs":[],"source":["# 1° Aligning labels and species\n","import pandas as pd\n","df = pd.DataFrame({'labels':labels, 'species':species})\n","\n","# 2° Crosstab od labels and species\n","ct = pd.crosstab(df['labels'], df['species'])"]},{"cell_type":"markdown","id":"83e5c646-de40-472f-bf06-384db311cf21","metadata":{},"source":["## Measuring clustering quality\n","- Using only samples and  their cluster labels\n","- A good clustering has tight clusters\n","- Samples in each cluster bunched together\n","\n","### Inertia measures clustering quality\n","- Measures how spread out the clusters are (lower is better)\n","- Distance from each sample to centroid of its cluster\n","- After **fit()**, available as attribute **inertia_**"]},{"cell_type":"code","execution_count":null,"id":"dc731bd7-da96-4d8b-89c9-b16e55d41970","metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\n","model = KMeans(n_clusters=3)\n","model.fit(samples)\n","print(model.inertia_)"]},{"cell_type":"markdown","id":"88a9b30e-ffd3-45c7-b996-e4f1202146b3","metadata":{},"source":["## Transforming features for better clusterings"]},{"cell_type":"code","execution_count":null,"id":"6440ab16-7007-4870-9191-1ea4134448ea","metadata":{},"outputs":[],"source":["# StandardScaler\n","from sklearn.preprocessing import Standard Scaler\n","scaler = StandardScaler()\n","scaler.fit(samples)\n","StandardScaler(copy=True, with_mean=True, with_std=True)\n","samples_scaled = scaler.transform(samples)"]},{"cell_type":"code","execution_count":null,"id":"aa05536f-1fa3-451a-ba4f-78b7cda79de2","metadata":{},"outputs":[],"source":["# Pipelines combine multiple steps\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","scaler = StandardScaler()\n","kmeans = KMeans(n_clusters=3)\n","\n","from sklearn.pipeline import make_pipeline\n","pipeline = make_pipeline(scaler, kmeans)\n","pipeline.fit(samples)\n","\n","labels = pipeline.predict(samples)"]},{"cell_type":"markdown","id":"94ea2708-4bf0-4e10-8c4e-7a70e82f917d","metadata":{},"source":["## Visualizing hierarchies"]},{"cell_type":"code","execution_count":null,"id":"3bc543e6-b253-4c59-b697-a9d51714343a","metadata":{},"outputs":[],"source":["# Hierarchical clustering with SciPy\n","import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","mergings = linkage(samples, method='complete')\n","dendrogram(mergings=country_names,\n","         leaf_rotation=90,\n","         leaf_front_size=6)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"92bdccf3-b570-4e56-b8d5-6d2df8077b85","metadata":{},"outputs":[],"source":["# Extracting cluster labels using fcluster\n","from scipy.cluster.hierarchy import linkage\n","mergings = linkage(samples, method='complete')\n","from scipy.cluster.hierarchy import fcluster\n","labels = fcluster(mergings, 15, criterion='distance')\n","print(labels)"]},{"cell_type":"code","execution_count":null,"id":"d74cdffd-d1a7-4171-a0e3-f199fbaccb93","metadata":{},"outputs":[],"source":["# Aligning cluster labels with country names\n","import pandas as pd\n","pairs = pd.DataFrame({'labels':labels, 'countries': country_names})\n","print(pairs.sort_values('labels'))"]},{"cell_type":"markdown","id":"7d76613f-0dd3-42aa-bea4-65a868878061","metadata":{},"source":["## t-SNE for 2-dimensional maps\n","- t-SNE = \"t-distributed stochastic neighbor embedding\"\n","- Maps samples to 2D space (or 3D)\n","- Map approximately preserves nearness of samples\n","- Great for inspecting datasets"]},{"cell_type":"code","execution_count":null,"id":"3254ea91-7b64-44b0-a4c9-c70bce00db09","metadata":{},"outputs":[],"source":["# t-SNE in sklearn\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","model = TSNE(learning_rate=100)\n","transformed = model.fit_transform(samples)\n","xs = transformed[:,0]\n","ys = transformed[:,1]\n","plt.scatter(xs, ys, c=species)\n","plt.show()"]},{"cell_type":"markdown","id":"22a5ec22-8d19-4c72-8d0b-4cf485c49bb8","metadata":{},"source":["## Principal Component Analysis (PCA)\n","- Fundamental dimension reduction technique:\n","1. More efficient storage and computation\n","2. Remove less-informative \"noise\" features\n","3. ...which cause problems for prediction tasks, e.g. classification, regression\n","\n","### PCA aligns data with axes\n","- Rotates data samples to be aligned with axes\n","- Shifts data samples so they have mean 0\n","- No information is lost\n","\n","**Obs**: PCA is called \"Principal component analysis\" because it learns the \"principal components\" of the data\n","- \"Principal components\" = directions of variance\n","- PCA aligns principal components with the axes"]},{"cell_type":"code","execution_count":null,"id":"b0719c40-f545-468a-a2f3-c46eaad69b31","metadata":{},"outputs":[],"source":["# Using scikit-learn PCA\n","from sklearn.decomposition import PCA\n","model = PCA()\n","model.fit(samples)\n","transformed = model.transform(samples)\n","print(model.components_)"]},{"cell_type":"markdown","id":"3c93fd4e-9a59-4302-991a-91935a0440cc","metadata":{},"source":["### Intrinsic dimension = number of features needed to approximate the dataset\n","### Intrinsic dimension = number of PCA features with significant variance\n","- Essential idea behind dimension reduction\n","- What is the most compact representation of the samples"]},{"cell_type":"code","execution_count":null,"id":"35089048-9742-4efc-9741-c8d63f60e4f6","metadata":{},"outputs":[],"source":["# Can use scipy.sparse.csr_matrix instead NumPy array\n","from sklearn.decomposition import TruncatedSVD\n","model = TruncatedSVD(n_components=3)\n","model.fit(documents)\n","transformed = model.transform(documents)"]},{"cell_type":"markdown","id":"113335cc-e14b-438e-8837-ca2d6743f100","metadata":{},"source":["## Non-negative matrix factorization (NMF)"]},{"cell_type":"code","execution_count":null,"id":"0a9d86cd-7995-47a9-9631-4340bbd109b7","metadata":{},"outputs":[],"source":["# NMF\n","from sklearn.decomposition import NMF\n","model = NMF(n_components=2)\n","model.fit(samples)\n","nmf_features = model.transform(samples)\n","\n","# Visualizing samples\n","bitmap = sample.reshape((2, 3))\n","from matplotlib import pyplot as plt\n","plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n","plt.show()"]},{"cell_type":"markdown","id":"385d7337-3159-44cf-b95d-8878541b8af0","metadata":{},"source":["# Machine Learning with Tree-Based Models in Python"]},{"cell_type":"markdown","id":"656189e7-a6bd-4601-be5b-45944d131bfb","metadata":{},"source":["## Classification-Tees\n","\n","### General aspects\n","- Sequence of if-else questions about individual features.\n","- **Objective**: infer class labels.\n","- Able to capture non-linear relationships between features and labels.\n","- Don't require feature scaling (ex:Standardization,..)\n","\n","### Decisions Regions\n","- Region in the feature space where all instances are assigned to one class label\n","\n","### Decision Boundary\n","- Surface separating different decision regions\n","\n","### Building blocks of a Decision-Tree\n","- **Decision-Tree**: data structure consisting of a hierarchy of nodes\n","- **Node**: question or prediction\n","- Three kinds of nodes:\n","1. **Root**: no parent node, question giving rise to two children nodes\n","2. **Internal node**: one parent node, question giving rise to two children nodes.\n","3. **Leaf**: one parent node, no children nodes --> prediction"]},{"cell_type":"code","execution_count":null,"id":"caedbae9-7b31-4141-909b-25430ec76ba8","metadata":{},"outputs":[],"source":["# Decision tree for classification\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, \n","                                                   stratify=y, random_state=1)\n","dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n","# dt = DecisionTreeClassifier(criterion='gini', random_state=1)\n","dt.fit(X_train,y_train) \n","y_pred = dt.predict(X_test)\n","\n","accuracy_score(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"id":"4eb2854b-7973-4e09-88e9-0c9ba78b0995","metadata":{},"outputs":[],"source":["# Decision tree for regression\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error as MSE\n","\n","X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=3)\n","\n","dt = DecisionTreeRegressor(max_depth=4,                            \n","                           min_samples_leaf=0.1,                           \n","                           random_state=3)\n","dt.fit(X_train,y_train) \n","y_pred = dt.predict(X_test)\n","\n","mse_dt = MSE(y_test, y_pred)\n","rmse_dt = mse_dt**(1/2)\n","print(rmse_dt)"]},{"cell_type":"markdown","id":"48e9a793-dd7a-4f8a-8fa0-fbc2e2cc677a","metadata":{},"source":["## The Bias-Variance Tradeoff\n","\n","### Difficulties in Approximating f\n","- **Overfitting:** 'f'(x) fits the training set noise\n","- **Underfitting:** 'f' is not flexible enough to approximate f.\n","\n","## Generalization Error\n","'f' = bias² + variance + irreducible error\n","\n","### Bias\n","- **Bias:** error term that tells you, on average, how much 'f' ≠ f\n","\n","### Variance\n","- **Variance:** tells you how much 'f' is inconsistent over different training sets\n","\n","### Model Complexity\n","- **Model Complexity:** sets the flexibility of 'f'\n","![1](1.webp)"]},{"cell_type":"code","execution_count":null,"id":"7dbeff2d-e623-4f9a-9257-abe2b466a8f4","metadata":{},"outputs":[],"source":["# K-Fold CV\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error as MSE\n","from sklearn.model_selection import cross_val_score\n","\n","SEED = 123\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                    random_state=SEED)\n","dt = DecisionTreeRegressor(max_depth=4, \n","                           min_samples_leaf=0.14, \n","                           random_state=SEED)\n","\n","MSE_CV = - cross_val_score(dt, X_train, y_train, cv= 10,\n","                           scoring='neg_mean_squared_error',\n","                           n_jobs = -1)\n","dt.fit(X_train, y_train)\n","y_predict_train = dt.predict(X_train)\n","y_predict_test = dt.predict(X_test)\n","\n","# CV MSE\n","print('CV MSE: {:.2f}'.format(MSE_CV.mean()))\n","# Training set MSE\n","print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))\n","# Test set MSE\n","print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))"]},{"cell_type":"markdown","id":"b36c4f26-241f-4cff-bfb2-6155e8b40ad8","metadata":{},"source":["## Ensemble Learnig\n","- Train dierent models on the same dataset.\n","- Let each model make its predictions.\n","- Meta-model: aggregates predictions of individual models.\n","- Final prediction: more robust and less prone to errors.\n","- Best results: models are skillful in dierent ways."]},{"cell_type":"code","execution_count":null,"id":"05443ef8-fdb7-4480-8fd5-45f4dd7e6e7f","metadata":{},"outputs":[],"source":["# Voting Classfier in sklearn\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier as KNN\n","from sklearn.ensemble import VotingClassifier\n","\n","SEED = 1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3,\n","                                                    random_state= SEED)\n","lr = LogisticRegression(random_state=SEED)\n","knn = KNN()\n","dt = DecisionTreeClassifier(random_state=SEED)\n","classifiers = [('Logistic Regression', lr),\n","               ('K Nearest Neighbours', knn),\n","               ('Classification Tree', dt)]\n","\n","# Iterate over the defined list of tuples containing the classifiers\n","for clf_name, clf in classifiers:\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n","\n","# Instantiate a VotingClassifier 'vc'\n","vc = VotingClassifier(estimators=classifiers) \n","vc.fit(X_train, y_train)\n","y_pred = vc.predict(X_test)\n","print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))"]},{"cell_type":"markdown","id":"83717f7d-5729-4e74-987e-e0bbba0767bf","metadata":{},"source":["## Bagging\n","- Bagging: Bootstrap Aggregation . \n","- Uses a technique known as the bootstrap. \n","- Reduces variance of individual models in the ensemble."]},{"cell_type":"code","execution_count":null,"id":"b76ad50c-6cde-4291-b76f-99502605812a","metadata":{},"outputs":[],"source":["# Bagging Classifier in sklearn\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","SEED = 1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n","stratify=y, random_state=SEED)\n","\n","dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n","bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n","\n","bc.fit(X_train, y_train)\n","y_pred = bc.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))"]},{"cell_type":"markdown","id":"6f980572-7a67-46f0-95f5-dcd8cefd09d5","metadata":{},"source":["## Random Forest Regressor"]},{"cell_type":"code","execution_count":null,"id":"a3e28a8a-56a5-4ff4-972f-0f7b759a2d83","metadata":{},"outputs":[],"source":["# Random Forest Regressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error as MSE\n","\n","SEED = 1\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                    random_state=SEED)\n","f = RandomForestRegressor(n_estimators=400,\n","                          min_samples_leaf=0.12,\n","                          random_state=SEED)\n","rf.fit(X_train, y_train)\n","y_pred = rf.predict(X_test)\n","rmse_test = MSE(y_test, y_pred)**(1/2)\n","print('Test set RMSE of rf: {:.2f}'.format(rmse_test)"]},{"cell_type":"code","execution_count":null,"id":"fbf7f399-84be-4239-a4aa-f562c45a05dc","metadata":{},"outputs":[],"source":["# Feature Importance in sklearn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n","sorted_importances_rf = importances_rf.sort_values()\n","sorted_importances_rf.plot(kind='barh', color='lightgreen')\n","plt.show()"]},{"cell_type":"markdown","id":"aa9b59df-9cdf-45e5-965f-adb1d59db8e9","metadata":{},"source":["## AdaBoost (Adaptative Boosting)\n","- Train an ensemble of predictors sequentially.\n","- Each predictor tries to correct its predecessor.\n","- Each predictor pays more attention to the instances wrongly predicted by its predecessor.\n","- Achieved by changing the weights of training instances.\n","- Each predictor is assigned a coefficient _a_.\n","- _a_ depends on the predictor training error."]},{"cell_type":"code","execution_count":null,"id":"02c4cec7-0a93-4de2-a935-f517fb27e591","metadata":{},"outputs":[],"source":["# AdaBoost Classification in sklearn\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","\n","SEED = 1\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n","                                                    stratify=y,\n","                                                    random_state=SEED)\n","dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n","adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n","\n","adb_clf.fit(X_train, y_train)\n","y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n","adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n","\n","print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))"]},{"cell_type":"markdown","id":"45e43361-715a-4dad-89b0-0628c9c5c169","metadata":{},"source":["## Gradient Boosting (GB)\n","- Sequential correction of predecessor's erros.\n","- Does not tweak the weights of training intances.\n","- Fit each predictor is trained using its predecessor's residual errors as labels.\n","- Gradient Boosted Trees: a CART is used as a base learner."]},{"cell_type":"code","execution_count":null,"id":"6e9bc666-0b57-43a6-bae6-8d3210ab02c9","metadata":{},"outputs":[],"source":["# Gradient Boosting in sklearn\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error as MSE\n","\n","SEED = 1\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                    random_state=SEED)\n","\n","gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, \n","                                random_state=SEED)\n","gbt.fit(X_train, y_train)\n","y_pred = gbt.predict(X_test)\n","rmse_test = MSE(y_test, y_pred)**(1/2)\n","print('Test set RMSE: {:.2f}'.format(rmse_test)"]},{"cell_type":"markdown","id":"ef0b20fd-80ed-4cbf-848a-d903d5408959","metadata":{},"source":["## Stochastic Gradient Boosting (SGB)\n","- Each tree is trained on a random subset of rows of the training data.\n","- The sampled instances (40% - 80% of the training set) are sampled without replacement.\n","- Features are sampled (without replacement) when choosing split points.\n","- Result: further ensemble diversity\n","- Effect: adding further variance to the ensemble of trees"]},{"cell_type":"code","execution_count":null,"id":"1500bf5d-7794-4d6a-8f4a-1105889a41be","metadata":{},"outputs":[],"source":["# Stochastic Gradient Boosting in sklearn\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error as MSE\n","\n","SEED = 1\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n","                                                    random_state=SEED)\n","sgbt = GradientBoostingRegressor(max_depth=1, \n","                                 subsample=0.8,\n","                                 max_features=0.2,\n","                                 n_estimators=300,\n","                                 random_state=SEED)\n","gbt.fit(X_train, y_train)\n","y_pred = sgbt.predict(X_test)\n","\n","rmse_test = MSE(y_test, y_pred)**(1/2)\n","print('Test set RMSE: {:.2f}'.format(rmse_test))"]},{"cell_type":"markdown","id":"864e3239-4799-422e-abbc-85ef10bf2a0f","metadata":{},"source":["## Hyperparameters\n","- **Parameters:** learnd from data\n","- **Hyperparameters:** not learned from data, set prior to training"]},{"cell_type":"code","execution_count":null,"id":"6fd84d18-a827-4add-b48b-83f033e1366a","metadata":{},"outputs":[],"source":["# Inspecting the hyperparameters of a CART in sklearn\n","from sklearn.tree import DecisionTreeClassifier\n","\n","SEED = 1\n","dt = DecisionTreeClassifier(random_state=SEED)\n","print(dt.get_params())"]},{"cell_type":"code","execution_count":null,"id":"421fbf51-1e03-42c9-9fab-51ac7069a7df","metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","params_dt = {\n","    'max_depth': [3, 4, 5, 6], \n","    'min_samples_leaf': [0.04, 0.06, 0.08], \n","    'max_features': [0.2, 0.4, 0.6, 0.8]\n","}\n","grid_dt = GridSearchCV(estimator=dt,\n","                       param_grid=params_dt, \n","                       scoring='accuracy', \n","                       cv=10, \n","                       n_jobs=-1)\n","grid_dt.fit(X_train, y_train)\n","\n","# Extracting the best hyperparameters\n","best_hyperparams = grid_dt.best_params_\n","print('Best hyerparameters:\\n', best_hyperparams)\n","best_CV_score = grid_dt.best_score_\n","print('Best CV accuracy'.format(best_CV_score))Best CV accuracy: 0.938\n","\n","# Extracting the best estimator\n","best_model = grid_dt.best_estimator_\n","test_acc = best_model.score(X_test,y_test)\n","print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))"]},{"cell_type":"code","execution_count":null,"id":"a645e739-2359-4dd8-b89c-879d4b62f597","metadata":{},"outputs":[],"source":["# Inspecting Random Forest Hyperparameters in sklean\n","from sklearn.ensemble import RandomForestRegressor\n","\n","SEED = 1\n","rf = RandomForestRegressor(random_state= SEED)\n","print(rf.get_params())"]},{"cell_type":"code","execution_count":null,"id":"2f1d425e-78ea-4d80-a562-8973fb3c335a","metadata":{},"outputs":[],"source":["from sklearn.metrics import mean_squared_error as MSE\n","from sklearn.model_selection import GridSearchCV\n","params_rf = {\n","    'n_estimators': [300, 400, 500], \n","    'max_depth': [4, 6, 8], \n","    'min_samples_leaf': [0.1, 0.2], \n","    'max_features': ['log2', 'sqrt']\n","}\n","grid_rf = GridSearchCV(estimator=rf,\n","                       param_grid=params_rf,\n","                       cv=3, \n","                       scoring='neg_mean_squared_error', \n","                       verbose=1, \n","                       n_jobs=-1)\n","grid_rf.fit(X_train, y_train)\n","# Extracting the best hyperparameters\n","best_hyperparams = grid_rf.best_params_\n","print('Best hyperparameters:\\n', best_hyperparams)\n","\n","# Evaluating the model performance\n","best_model = grid_rf.best_estimator_\n","y_pred = best_model.predict(X_test)\n","rmse_test = MSE(y_test, y_pred)**(1/2)\n","print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
